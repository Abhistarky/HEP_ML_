{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph Neural Network_Abhisek_praharaj.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTING the LIBRARIES"
      ],
      "metadata": {
        "id": "SppibVQXO-Sl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F_eewmeIOZY_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd.variable import *\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "ORhdzN-DOtZG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "cY-_cNXFO7AM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparation of training and validation sample/\n"
      ],
      "metadata": {
        "id": "BlfQuJnpO8vt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Abhistarky/HEP_ML_.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjlH_UeGPJUl",
        "outputId": "866c0e1d-2854-4e02-95c3-6e66117af16c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'HEP_ML_' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls HEP_ML_/Data/JetDataset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6mlFP1JPS-V",
        "outputId": "e98a5530-34b5-43be-9386-7745dc46b578"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jetImage_7_100p_0_10000.h5\tjetImage_7_100p_50000_60000.h5\n",
            "jetImage_7_100p_10000_20000.h5\tjetImage_7_100p_60000_70000.h5\n",
            "jetImage_7_100p_30000_40000.h5\tjetImage_7_100p_70000_80000.h5\n",
            "jetImage_7_100p_40000_50000.h5\tjetImage_7_100p_80000_90000.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target, jetList = np.array([]), np.array([])\n",
        "datafiles =  ['HEP_ML_/Data/JetDataset/jetImage_7_100p_30000_40000.h5',\n",
        "            'HEP_ML_/Data/JetDataset/jetImage_7_100p_60000_70000.h5',\n",
        "            'HEP_ML_/Data/JetDataset/jetImage_7_100p_50000_60000.h5',\n",
        "            'HEP_ML_/Data/JetDataset/jetImage_7_100p_10000_20000.h5',\n",
        "            'HEP_ML_/Data/JetDataset/jetImage_7_100p_0_10000.h5']\n",
        "for fileIN in datafiles:\n",
        "  print(\"Appending %s\" %fileIN)\n",
        "  f= h5py.File(fileIN)\n",
        "  myJetList = np.array(f.get(\"jetConstituentList\"))\n",
        "  mytarget = np.array(f.get('jets')[0:, -6:-1])\n",
        "  jetList = np.concatenate([jetList, myJetList], axis= 0) if jetList.size else myJetList\n",
        "  target = np.concatenate([target, mytarget], axis= 0)if target.size else mytarget\n",
        "  del myJetList, mytarget\n",
        "  f.close()\n",
        "print(target.shape, jetList.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAkAbHacPgPD",
        "outputId": "8a7f01b6-4f02-46c3-81db-7eb4960f9087"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending HEP_ML_/Data/JetDataset/jetImage_7_100p_30000_40000.h5\n",
            "Appending HEP_ML_/Data/JetDataset/jetImage_7_100p_60000_70000.h5\n",
            "Appending HEP_ML_/Data/JetDataset/jetImage_7_100p_50000_60000.h5\n",
            "Appending HEP_ML_/Data/JetDataset/jetImage_7_100p_10000_20000.h5\n",
            "Appending HEP_ML_/Data/JetDataset/jetImage_7_100p_0_10000.h5\n",
            "(50000, 5) (50000, 100, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists of 50K with up to 100 particles in each jet. These 100 particles have been used to fill the 100x100 jet images."
      ],
      "metadata": {
        "id": "PmKZxNWHTCiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch Cross Entropy doesn't support one-hot encoding\n",
        "target = np.argmax(target, axis=1)\n",
        "# the dataset is N_jets x N_particles x N_features\n",
        "# the IN wants N_jets x N_features x N_particles\n",
        "jetList = np.swapaxes(jetList, 1, 2)"
      ],
      "metadata": {
        "id": "aRiWFwOsQ7sF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we will shuffle the data , split it for training and testing."
      ],
      "metadata": {
        "id": "ISlrJZvqTMTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nParticle = 30\n",
        "jetList = jetList[:,:,:nParticle]\n",
        "print(jetList.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0thGfzNSTKfD",
        "outputId": "401930a8-30cd-446e-fb76-7430b2b2258a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 16, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(jetList, target, test_size=0.33)\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
        "del jetList, target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyCTO59rTZ-j",
        "outputId": "fc7e3c5a-c555-46d8-f82a-b292e6b73cbb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(33500, 16, 30) (16500, 16, 30) (33500,) (16500,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if a GPU is available. Otherwise run on CPU\n",
        "device = 'cpu'\n",
        "args_cuda = torch.cuda.is_available()\n",
        "if args_cuda: device = \"cuda\"\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNm2S-qVTd3j",
        "outputId": "24806d2f-3a23-405c-9bae-ae0929405c7c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dataset to pytorch\n",
        "X_train = Variable(torch.FloatTensor(X_train)).to(device)\n",
        "X_val = Variable(torch.FloatTensor(X_val)).to(device)\n",
        "y_train = Variable(torch.LongTensor(y_train).long()).to(device)\n",
        "y_val = Variable(torch.LongTensor(y_val).long()).to(device)"
      ],
      "metadata": {
        "id": "_8CYuHE5ThJr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THE ITERATIVE NETWORK MODEL"
      ],
      "metadata": {
        "id": "t9fWqxRxT6r7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "class GraphNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GraphNet, self).__init__()\n",
        "        self.P = 16 # number of features\n",
        "        self.N = nParticle # number of particles\n",
        "        self.Nr = self.N * (self.N - 1)\n",
        "        self.De = 8 # dimensionality of De learned representation\n",
        "        self.Do = 8 # number of engineered features\n",
        "        self.n_targets = 5 # number of target classes\n",
        "        self.assign_matrices() # build Rr and Rs\n",
        "\n",
        "        # build netwok\n",
        "        self.batchnorm_x = nn.BatchNorm1d(self.P)\n",
        "        self.hidden = 10\n",
        "        self.fr1 = nn.Linear(2 * self.P, self.hidden).to(device)\n",
        "        self.fr2 = nn.Linear(self.hidden, self.hidden).to(device)\n",
        "        self.fr3 = nn.Linear(self.hidden, self.De).to(device)\n",
        "        self.fo1 = nn.Linear(self.P + self.De, self.hidden).to(device)\n",
        "        self.fo2 = nn.Linear(self.hidden, self.hidden).to(device)\n",
        "        self.fo3 = nn.Linear(self.hidden, self.Do).to(device)\n",
        "        self.fc1 = nn.Linear(self.Do, self.hidden).to(device)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden).to(device)\n",
        "        self.fc3 = nn.Linear(self.hidden, self.n_targets).to(device)\n",
        "             \n",
        "    def assign_matrices(self):\n",
        "        self.Rr = torch.zeros(self.N, self.Nr)\n",
        "        self.Rs = torch.zeros(self.N, self.Nr)\n",
        "        receiver_sender_list = [i for i in itertools.product(range(self.N), range(self.N)) if i[0]!=i[1]]\n",
        "        for i, (r, s) in enumerate(receiver_sender_list):\n",
        "            self.Rr[r, i] = 1\n",
        "            self.Rs[s, i] = 1\n",
        "        self.Rr = Variable(self.Rr).to(device)\n",
        "        self.Rs = Variable(self.Rs).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.batchnorm_x(x) # [batch, P, N]\n",
        "        Orr = self.tmul(x, self.Rr)\n",
        "        Ors = self.tmul(x, self.Rs)\n",
        "        B = torch.cat([Orr, Ors], 1)\n",
        "        ### First MLP ###\n",
        "        B = torch.transpose(B, 1, 2).contiguous()\n",
        "        B = nn.functional.relu(self.fr1(B.view(-1, 2 * self.P)))\n",
        "        B = nn.functional.relu(self.fr2(B))\n",
        "        E = nn.functional.relu(self.fr3(B).view(-1, self.Nr, self.De))\n",
        "        del B\n",
        "        E = torch.transpose(E, 1, 2).contiguous()\n",
        "        Ebar = self.tmul(E, torch.transpose(self.Rr, 0, 1).contiguous())\n",
        "        del E\n",
        "        C = torch.cat([x, Ebar], 1)\n",
        "        del Ebar\n",
        "        C = torch.transpose(C, 1, 2).contiguous()\n",
        "        ### Second MLP ###\n",
        "        C = nn.functional.relu(self.fo1(C.view(-1, self.P + self.De)))\n",
        "        C = nn.functional.relu(self.fo2(C))\n",
        "        O = nn.functional.relu(self.fo3(C).view(-1, self.N, self.Do))\n",
        "        del C\n",
        "        # sum over constituents\n",
        "        O = torch.sum(O,1)\n",
        "        ### Classification MLP ###\n",
        "        N = nn.functional.relu(self.fc1(O.view(-1, self.Do)))\n",
        "        N = nn.functional.relu(self.fc2(N))\n",
        "        del O\n",
        "        N = self.fc3(N)\n",
        "        return N\n",
        "\n",
        "    def tmul(self, x, y):  #Takes (I * J * K)(K * L) -> I * J * L \n",
        "        x_shape = x.size()\n",
        "        y_shape = y.size()\n",
        "        prod = torch.mm(x.reshape(x_shape[0]*x_shape[1], x_shape[2]), y).view(-1, x_shape[1], y_shape[1])\n",
        "        return prod\n",
        "\n",
        "def get_sample(training, target, choice):\n",
        "    target_vals = np.argmax(target, axis = 1)\n",
        "    ind, = np.where(target_vals == choice)\n",
        "    chosen_ind = np.random.choice(ind, 50000)\n",
        "    return training[chosen_ind], target[chosen_ind]"
      ],
      "metadata": {
        "id": "vRilP8aQT9O1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 800\n",
        "batch_size = 100\n",
        "patience =  10"
      ],
      "metadata": {
        "id": "f256CvrzT-IL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gnn = GraphNet()\n",
        "gnn.to(device)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(gnn.parameters(), lr = 0.0001)\n",
        "\n",
        "loss_train = np.zeros(n_epochs)\n",
        "acc_train = np.zeros(n_epochs)\n",
        "loss_val = np.zeros(n_epochs)\n",
        "acc_val = np.zeros(n_epochs)\n",
        "for i in range(n_epochs):\n",
        "    print(\"Epoch %s\" % i)\n",
        "    for j in range(0, X_train.size()[0], batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        out = gnn(X_train[j:j + batch_size,:,:])\n",
        "        target = y_train[j:j + batch_size]\n",
        "        l = loss(out, target)\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "        loss_train[i] += l.cpu().data.numpy()*batch_size\n",
        "    loss_train[i] = loss_train[i]/X_train.shape[0]\n",
        "    #acc_train[i] = stats(predicted, Y_val)\n",
        "    #### val loss & accuracy\n",
        "    for j in range(0, X_val.size()[0], batch_size):\n",
        "        out_val = gnn(X_val[j:j + batch_size])\n",
        "        target_val =  y_val[j:j + batch_size]\n",
        "        \n",
        "        l_val = loss(out_val,target_val)\n",
        "        loss_val[i] += l_val.cpu().data.numpy()*batch_size\n",
        "    loss_val[i] = loss_val[i]/X_val.shape[0]\n",
        "    print(\"Training   Loss: %f\" %l.cpu().data.numpy())\n",
        "    print(\"Validation Loss: %f\" %l_val.cpu().data.numpy())\n",
        "    if all(loss_val[max(0, i - patience):i] > min(np.append(loss_val[0:max(0, i - patience)], 200))) and i > patience:\n",
        "        print(\"Early Stopping\")\n",
        "        break\n",
        "    print"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXloqdXvUi88",
        "outputId": "761c2e41-fb5b-403f-f9ab-272406493d2f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Training   Loss: 1.576466\n",
            "Validation Loss: 1.592333\n",
            "Epoch 1\n",
            "Training   Loss: 1.356313\n",
            "Validation Loss: 1.402797\n",
            "Epoch 2\n",
            "Training   Loss: 1.221027\n",
            "Validation Loss: 1.276093\n",
            "Epoch 3\n",
            "Training   Loss: 1.162052\n",
            "Validation Loss: 1.210439\n",
            "Epoch 4\n",
            "Training   Loss: 1.133045\n",
            "Validation Loss: 1.182683\n",
            "Epoch 5\n",
            "Training   Loss: 1.117840\n",
            "Validation Loss: 1.168804\n",
            "Epoch 6\n",
            "Training   Loss: 1.106021\n",
            "Validation Loss: 1.157991\n",
            "Epoch 7\n",
            "Training   Loss: 1.095520\n",
            "Validation Loss: 1.148154\n",
            "Epoch 8\n",
            "Training   Loss: 1.086474\n",
            "Validation Loss: 1.139718\n",
            "Epoch 9\n",
            "Training   Loss: 1.075546\n",
            "Validation Loss: 1.130443\n",
            "Epoch 10\n",
            "Training   Loss: 1.064231\n",
            "Validation Loss: 1.123213\n",
            "Epoch 11\n",
            "Training   Loss: 1.054785\n",
            "Validation Loss: 1.119297\n",
            "Epoch 12\n",
            "Training   Loss: 1.045995\n",
            "Validation Loss: 1.118249\n",
            "Epoch 13\n",
            "Training   Loss: 1.036397\n",
            "Validation Loss: 1.116893\n",
            "Epoch 14\n",
            "Training   Loss: 1.027493\n",
            "Validation Loss: 1.116288\n",
            "Epoch 15\n",
            "Training   Loss: 1.019247\n",
            "Validation Loss: 1.114326\n",
            "Epoch 16\n",
            "Training   Loss: 1.010834\n",
            "Validation Loss: 1.113143\n",
            "Epoch 17\n",
            "Training   Loss: 1.003482\n",
            "Validation Loss: 1.110464\n",
            "Epoch 18\n",
            "Training   Loss: 0.996733\n",
            "Validation Loss: 1.110365\n",
            "Epoch 19\n",
            "Training   Loss: 0.991650\n",
            "Validation Loss: 1.109322\n",
            "Epoch 20\n",
            "Training   Loss: 0.987656\n",
            "Validation Loss: 1.108304\n",
            "Epoch 21\n",
            "Training   Loss: 0.984654\n",
            "Validation Loss: 1.108026\n",
            "Epoch 22\n",
            "Training   Loss: 0.982489\n",
            "Validation Loss: 1.107223\n",
            "Epoch 23\n",
            "Training   Loss: 0.980862\n",
            "Validation Loss: 1.105021\n",
            "Epoch 24\n",
            "Training   Loss: 0.979350\n",
            "Validation Loss: 1.104897\n",
            "Epoch 25\n",
            "Training   Loss: 0.978976\n",
            "Validation Loss: 1.102774\n",
            "Epoch 26\n",
            "Training   Loss: 0.978825\n",
            "Validation Loss: 1.101631\n",
            "Epoch 27\n",
            "Training   Loss: 0.977762\n",
            "Validation Loss: 1.100260\n",
            "Epoch 28\n",
            "Training   Loss: 0.977249\n",
            "Validation Loss: 1.098291\n",
            "Epoch 29\n",
            "Training   Loss: 0.975554\n",
            "Validation Loss: 1.096914\n",
            "Epoch 30\n",
            "Training   Loss: 0.973867\n",
            "Validation Loss: 1.095407\n",
            "Epoch 31\n",
            "Training   Loss: 0.972936\n",
            "Validation Loss: 1.093845\n",
            "Epoch 32\n",
            "Training   Loss: 0.972066\n",
            "Validation Loss: 1.092559\n",
            "Epoch 33\n",
            "Training   Loss: 0.971765\n",
            "Validation Loss: 1.091288\n",
            "Epoch 34\n",
            "Training   Loss: 0.971250\n",
            "Validation Loss: 1.090303\n",
            "Epoch 35\n",
            "Training   Loss: 0.970745\n",
            "Validation Loss: 1.088895\n",
            "Epoch 36\n",
            "Training   Loss: 0.970409\n",
            "Validation Loss: 1.087446\n",
            "Epoch 37\n",
            "Training   Loss: 0.969689\n",
            "Validation Loss: 1.085192\n",
            "Epoch 38\n",
            "Training   Loss: 0.968608\n",
            "Validation Loss: 1.084179\n",
            "Epoch 39\n",
            "Training   Loss: 0.967656\n",
            "Validation Loss: 1.082589\n",
            "Epoch 40\n",
            "Training   Loss: 0.966605\n",
            "Validation Loss: 1.081112\n",
            "Epoch 41\n",
            "Training   Loss: 0.966104\n",
            "Validation Loss: 1.078768\n",
            "Epoch 42\n",
            "Training   Loss: 0.965548\n",
            "Validation Loss: 1.077364\n",
            "Epoch 43\n",
            "Training   Loss: 0.965085\n",
            "Validation Loss: 1.074518\n",
            "Epoch 44\n",
            "Training   Loss: 0.964994\n",
            "Validation Loss: 1.072491\n",
            "Epoch 45\n",
            "Training   Loss: 0.964344\n",
            "Validation Loss: 1.071071\n",
            "Epoch 46\n",
            "Training   Loss: 0.963667\n",
            "Validation Loss: 1.068908\n",
            "Epoch 47\n",
            "Training   Loss: 0.962914\n",
            "Validation Loss: 1.066708\n",
            "Epoch 48\n",
            "Training   Loss: 0.962346\n",
            "Validation Loss: 1.064276\n",
            "Epoch 49\n",
            "Training   Loss: 0.961721\n",
            "Validation Loss: 1.062668\n",
            "Epoch 50\n",
            "Training   Loss: 0.960752\n",
            "Validation Loss: 1.060753\n",
            "Epoch 51\n",
            "Training   Loss: 0.960200\n",
            "Validation Loss: 1.058586\n",
            "Epoch 52\n",
            "Training   Loss: 0.959759\n",
            "Validation Loss: 1.056544\n",
            "Epoch 53\n",
            "Training   Loss: 0.958896\n",
            "Validation Loss: 1.053462\n",
            "Epoch 54\n",
            "Training   Loss: 0.957888\n",
            "Validation Loss: 1.051377\n",
            "Epoch 55\n",
            "Training   Loss: 0.956989\n",
            "Validation Loss: 1.048957\n",
            "Epoch 56\n",
            "Training   Loss: 0.955727\n",
            "Validation Loss: 1.047168\n",
            "Epoch 57\n",
            "Training   Loss: 0.955126\n",
            "Validation Loss: 1.044536\n",
            "Epoch 58\n",
            "Training   Loss: 0.954123\n",
            "Validation Loss: 1.042449\n",
            "Epoch 59\n",
            "Training   Loss: 0.953414\n",
            "Validation Loss: 1.039664\n",
            "Epoch 60\n",
            "Training   Loss: 0.952478\n",
            "Validation Loss: 1.036976\n",
            "Epoch 61\n",
            "Training   Loss: 0.950918\n",
            "Validation Loss: 1.033759\n",
            "Epoch 62\n",
            "Training   Loss: 0.949905\n",
            "Validation Loss: 1.032150\n",
            "Epoch 63\n",
            "Training   Loss: 0.949173\n",
            "Validation Loss: 1.029853\n",
            "Epoch 64\n",
            "Training   Loss: 0.947265\n",
            "Validation Loss: 1.027958\n",
            "Epoch 65\n",
            "Training   Loss: 0.946570\n",
            "Validation Loss: 1.025627\n",
            "Epoch 66\n",
            "Training   Loss: 0.944822\n",
            "Validation Loss: 1.023554\n",
            "Epoch 67\n",
            "Training   Loss: 0.944116\n",
            "Validation Loss: 1.021657\n",
            "Epoch 68\n",
            "Training   Loss: 0.942981\n",
            "Validation Loss: 1.019756\n",
            "Epoch 69\n",
            "Training   Loss: 0.941614\n",
            "Validation Loss: 1.017863\n",
            "Epoch 70\n",
            "Training   Loss: 0.940084\n",
            "Validation Loss: 1.016153\n",
            "Epoch 71\n",
            "Training   Loss: 0.938888\n",
            "Validation Loss: 1.014831\n",
            "Epoch 72\n",
            "Training   Loss: 0.937857\n",
            "Validation Loss: 1.013135\n",
            "Epoch 73\n",
            "Training   Loss: 0.936855\n",
            "Validation Loss: 1.011685\n",
            "Epoch 74\n",
            "Training   Loss: 0.935575\n",
            "Validation Loss: 1.010802\n",
            "Epoch 75\n",
            "Training   Loss: 0.934270\n",
            "Validation Loss: 1.009591\n",
            "Epoch 76\n",
            "Training   Loss: 0.933490\n",
            "Validation Loss: 1.007286\n",
            "Epoch 77\n",
            "Training   Loss: 0.932629\n",
            "Validation Loss: 1.005513\n",
            "Epoch 78\n",
            "Training   Loss: 0.932137\n",
            "Validation Loss: 1.003468\n",
            "Epoch 79\n",
            "Training   Loss: 0.931110\n",
            "Validation Loss: 1.001055\n",
            "Epoch 80\n",
            "Training   Loss: 0.931079\n",
            "Validation Loss: 0.999186\n",
            "Epoch 81\n",
            "Training   Loss: 0.930345\n",
            "Validation Loss: 0.997574\n",
            "Epoch 82\n",
            "Training   Loss: 0.929563\n",
            "Validation Loss: 0.995549\n",
            "Epoch 83\n",
            "Training   Loss: 0.929012\n",
            "Validation Loss: 0.994276\n",
            "Epoch 84\n",
            "Training   Loss: 0.928275\n",
            "Validation Loss: 0.992559\n",
            "Epoch 85\n",
            "Training   Loss: 0.927100\n",
            "Validation Loss: 0.991064\n",
            "Epoch 86\n",
            "Training   Loss: 0.926046\n",
            "Validation Loss: 0.989203\n",
            "Epoch 87\n",
            "Training   Loss: 0.925148\n",
            "Validation Loss: 0.987751\n",
            "Epoch 88\n",
            "Training   Loss: 0.924632\n",
            "Validation Loss: 0.986061\n",
            "Epoch 89\n",
            "Training   Loss: 0.923991\n",
            "Validation Loss: 0.984279\n",
            "Epoch 90\n",
            "Training   Loss: 0.923047\n",
            "Validation Loss: 0.982793\n",
            "Epoch 91\n",
            "Training   Loss: 0.922385\n",
            "Validation Loss: 0.981261\n",
            "Epoch 92\n",
            "Training   Loss: 0.921998\n",
            "Validation Loss: 0.979702\n",
            "Epoch 93\n",
            "Training   Loss: 0.921005\n",
            "Validation Loss: 0.978283\n",
            "Epoch 94\n",
            "Training   Loss: 0.920263\n",
            "Validation Loss: 0.977140\n",
            "Epoch 95\n",
            "Training   Loss: 0.919397\n",
            "Validation Loss: 0.976635\n",
            "Epoch 96\n",
            "Training   Loss: 0.918673\n",
            "Validation Loss: 0.975740\n",
            "Epoch 97\n",
            "Training   Loss: 0.918158\n",
            "Validation Loss: 0.974459\n",
            "Epoch 98\n",
            "Training   Loss: 0.917625\n",
            "Validation Loss: 0.973937\n",
            "Epoch 99\n",
            "Training   Loss: 0.916856\n",
            "Validation Loss: 0.973044\n",
            "Epoch 100\n",
            "Training   Loss: 0.916022\n",
            "Validation Loss: 0.972627\n",
            "Epoch 101\n",
            "Training   Loss: 0.915336\n",
            "Validation Loss: 0.971729\n",
            "Epoch 102\n",
            "Training   Loss: 0.913708\n",
            "Validation Loss: 0.971545\n",
            "Epoch 103\n",
            "Training   Loss: 0.912719\n",
            "Validation Loss: 0.970308\n",
            "Epoch 104\n",
            "Training   Loss: 0.911263\n",
            "Validation Loss: 0.969497\n",
            "Epoch 105\n",
            "Training   Loss: 0.910587\n",
            "Validation Loss: 0.968924\n",
            "Epoch 106\n",
            "Training   Loss: 0.910723\n",
            "Validation Loss: 0.968075\n",
            "Epoch 107\n",
            "Training   Loss: 0.909882\n",
            "Validation Loss: 0.966632\n",
            "Epoch 108\n",
            "Training   Loss: 0.909571\n",
            "Validation Loss: 0.964802\n",
            "Epoch 109\n",
            "Training   Loss: 0.908800\n",
            "Validation Loss: 0.963498\n",
            "Epoch 110\n",
            "Training   Loss: 0.908414\n",
            "Validation Loss: 0.961647\n",
            "Epoch 111\n",
            "Training   Loss: 0.907739\n",
            "Validation Loss: 0.959907\n",
            "Epoch 112\n",
            "Training   Loss: 0.907837\n",
            "Validation Loss: 0.958427\n",
            "Epoch 113\n",
            "Training   Loss: 0.907498\n",
            "Validation Loss: 0.955923\n",
            "Epoch 114\n",
            "Training   Loss: 0.906634\n",
            "Validation Loss: 0.953815\n",
            "Epoch 115\n",
            "Training   Loss: 0.905085\n",
            "Validation Loss: 0.951461\n",
            "Epoch 116\n",
            "Training   Loss: 0.904154\n",
            "Validation Loss: 0.949403\n",
            "Epoch 117\n",
            "Training   Loss: 0.903820\n",
            "Validation Loss: 0.947884\n",
            "Epoch 118\n",
            "Training   Loss: 0.903204\n",
            "Validation Loss: 0.945655\n",
            "Epoch 119\n",
            "Training   Loss: 0.902850\n",
            "Validation Loss: 0.943410\n",
            "Epoch 120\n",
            "Training   Loss: 0.902330\n",
            "Validation Loss: 0.941820\n",
            "Epoch 121\n",
            "Training   Loss: 0.901950\n",
            "Validation Loss: 0.939944\n",
            "Epoch 122\n",
            "Training   Loss: 0.901055\n",
            "Validation Loss: 0.939160\n",
            "Epoch 123\n",
            "Training   Loss: 0.900567\n",
            "Validation Loss: 0.937250\n",
            "Epoch 124\n",
            "Training   Loss: 0.899056\n",
            "Validation Loss: 0.935122\n",
            "Epoch 125\n",
            "Training   Loss: 0.898091\n",
            "Validation Loss: 0.933344\n",
            "Epoch 126\n",
            "Training   Loss: 0.897232\n",
            "Validation Loss: 0.931727\n",
            "Epoch 127\n",
            "Training   Loss: 0.895677\n",
            "Validation Loss: 0.930301\n",
            "Epoch 128\n",
            "Training   Loss: 0.894508\n",
            "Validation Loss: 0.929186\n",
            "Epoch 129\n",
            "Training   Loss: 0.893452\n",
            "Validation Loss: 0.927881\n",
            "Epoch 130\n",
            "Training   Loss: 0.890867\n",
            "Validation Loss: 0.927586\n",
            "Epoch 131\n",
            "Training   Loss: 0.889971\n",
            "Validation Loss: 0.926163\n",
            "Epoch 132\n",
            "Training   Loss: 0.888795\n",
            "Validation Loss: 0.924746\n",
            "Epoch 133\n",
            "Training   Loss: 0.888060\n",
            "Validation Loss: 0.924042\n",
            "Epoch 134\n",
            "Training   Loss: 0.887404\n",
            "Validation Loss: 0.923614\n",
            "Epoch 135\n",
            "Training   Loss: 0.886665\n",
            "Validation Loss: 0.922967\n",
            "Epoch 136\n",
            "Training   Loss: 0.885173\n",
            "Validation Loss: 0.921832\n",
            "Epoch 137\n",
            "Training   Loss: 0.884362\n",
            "Validation Loss: 0.920354\n",
            "Epoch 138\n",
            "Training   Loss: 0.883939\n",
            "Validation Loss: 0.919227\n",
            "Epoch 139\n",
            "Training   Loss: 0.882948\n",
            "Validation Loss: 0.918449\n",
            "Epoch 140\n",
            "Training   Loss: 0.881992\n",
            "Validation Loss: 0.917171\n",
            "Epoch 141\n",
            "Training   Loss: 0.881254\n",
            "Validation Loss: 0.916055\n",
            "Epoch 142\n",
            "Training   Loss: 0.880085\n",
            "Validation Loss: 0.915111\n",
            "Epoch 143\n",
            "Training   Loss: 0.879140\n",
            "Validation Loss: 0.913589\n",
            "Epoch 144\n",
            "Training   Loss: 0.878197\n",
            "Validation Loss: 0.912828\n",
            "Epoch 145\n",
            "Training   Loss: 0.876514\n",
            "Validation Loss: 0.911455\n",
            "Epoch 146\n",
            "Training   Loss: 0.875283\n",
            "Validation Loss: 0.910240\n",
            "Epoch 147\n",
            "Training   Loss: 0.873752\n",
            "Validation Loss: 0.909451\n",
            "Epoch 148\n",
            "Training   Loss: 0.872847\n",
            "Validation Loss: 0.908982\n",
            "Epoch 149\n",
            "Training   Loss: 0.871380\n",
            "Validation Loss: 0.908552\n",
            "Epoch 150\n",
            "Training   Loss: 0.870761\n",
            "Validation Loss: 0.908053\n",
            "Epoch 151\n",
            "Training   Loss: 0.869483\n",
            "Validation Loss: 0.907158\n",
            "Epoch 152\n",
            "Training   Loss: 0.867453\n",
            "Validation Loss: 0.906188\n",
            "Epoch 153\n",
            "Training   Loss: 0.865611\n",
            "Validation Loss: 0.905802\n",
            "Epoch 154\n",
            "Training   Loss: 0.863959\n",
            "Validation Loss: 0.904802\n",
            "Epoch 155\n",
            "Training   Loss: 0.862245\n",
            "Validation Loss: 0.904489\n",
            "Epoch 156\n",
            "Training   Loss: 0.860644\n",
            "Validation Loss: 0.903690\n",
            "Epoch 157\n",
            "Training   Loss: 0.858584\n",
            "Validation Loss: 0.903203\n",
            "Epoch 158\n",
            "Training   Loss: 0.856300\n",
            "Validation Loss: 0.903103\n",
            "Epoch 159\n",
            "Training   Loss: 0.854209\n",
            "Validation Loss: 0.902969\n",
            "Epoch 160\n",
            "Training   Loss: 0.851779\n",
            "Validation Loss: 0.902898\n",
            "Epoch 161\n",
            "Training   Loss: 0.849690\n",
            "Validation Loss: 0.902564\n",
            "Epoch 162\n",
            "Training   Loss: 0.847400\n",
            "Validation Loss: 0.902363\n",
            "Epoch 163\n",
            "Training   Loss: 0.844955\n",
            "Validation Loss: 0.902288\n",
            "Epoch 164\n",
            "Training   Loss: 0.842820\n",
            "Validation Loss: 0.902855\n",
            "Epoch 165\n",
            "Training   Loss: 0.840013\n",
            "Validation Loss: 0.903656\n",
            "Epoch 166\n",
            "Training   Loss: 0.838223\n",
            "Validation Loss: 0.904055\n",
            "Epoch 167\n",
            "Training   Loss: 0.835411\n",
            "Validation Loss: 0.905396\n",
            "Epoch 168\n",
            "Training   Loss: 0.833335\n",
            "Validation Loss: 0.905573\n",
            "Epoch 169\n",
            "Training   Loss: 0.831568\n",
            "Validation Loss: 0.905828\n",
            "Epoch 170\n",
            "Training   Loss: 0.829176\n",
            "Validation Loss: 0.905666\n",
            "Epoch 171\n",
            "Training   Loss: 0.826561\n",
            "Validation Loss: 0.907347\n",
            "Epoch 172\n",
            "Training   Loss: 0.824658\n",
            "Validation Loss: 0.907139\n",
            "Epoch 173\n",
            "Training   Loss: 0.823033\n",
            "Validation Loss: 0.907575\n",
            "Epoch 174\n",
            "Training   Loss: 0.821064\n",
            "Validation Loss: 0.908009\n",
            "Epoch 175\n",
            "Training   Loss: 0.819699\n",
            "Validation Loss: 0.907656\n",
            "Epoch 176\n",
            "Training   Loss: 0.817769\n",
            "Validation Loss: 0.907507\n",
            "Epoch 177\n",
            "Training   Loss: 0.815808\n",
            "Validation Loss: 0.907912\n",
            "Epoch 178\n",
            "Training   Loss: 0.814785\n",
            "Validation Loss: 0.908181\n",
            "Epoch 179\n",
            "Training   Loss: 0.813013\n",
            "Validation Loss: 0.908706\n",
            "Epoch 180\n",
            "Training   Loss: 0.811399\n",
            "Validation Loss: 0.909133\n",
            "Epoch 181\n",
            "Training   Loss: 0.809169\n",
            "Validation Loss: 0.909409\n",
            "Epoch 182\n",
            "Training   Loss: 0.807544\n",
            "Validation Loss: 0.910022\n",
            "Epoch 183\n",
            "Training   Loss: 0.805168\n",
            "Validation Loss: 0.910319\n",
            "Epoch 184\n",
            "Training   Loss: 0.803339\n",
            "Validation Loss: 0.910623\n",
            "Epoch 185\n",
            "Training   Loss: 0.801444\n",
            "Validation Loss: 0.910876\n",
            "Epoch 186\n",
            "Training   Loss: 0.799458\n",
            "Validation Loss: 0.911506\n",
            "Epoch 187\n",
            "Training   Loss: 0.797683\n",
            "Validation Loss: 0.912352\n",
            "Epoch 188\n",
            "Training   Loss: 0.796315\n",
            "Validation Loss: 0.912587\n",
            "Epoch 189\n",
            "Training   Loss: 0.794714\n",
            "Validation Loss: 0.913045\n",
            "Epoch 190\n",
            "Training   Loss: 0.793330\n",
            "Validation Loss: 0.913492\n",
            "Epoch 191\n",
            "Training   Loss: 0.791612\n",
            "Validation Loss: 0.913364\n",
            "Epoch 192\n",
            "Training   Loss: 0.790236\n",
            "Validation Loss: 0.913251\n",
            "Epoch 193\n",
            "Training   Loss: 0.788683\n",
            "Validation Loss: 0.913562\n",
            "Epoch 194\n",
            "Training   Loss: 0.787887\n",
            "Validation Loss: 0.913589\n",
            "Epoch 195\n",
            "Training   Loss: 0.786422\n",
            "Validation Loss: 0.913518\n",
            "Epoch 196\n",
            "Training   Loss: 0.785289\n",
            "Validation Loss: 0.913777\n",
            "Epoch 197\n",
            "Training   Loss: 0.784034\n",
            "Validation Loss: 0.914084\n",
            "Epoch 198\n",
            "Training   Loss: 0.783032\n",
            "Validation Loss: 0.915087\n",
            "Epoch 199\n",
            "Training   Loss: 0.780568\n",
            "Validation Loss: 0.915287\n",
            "Epoch 200\n",
            "Training   Loss: 0.779162\n",
            "Validation Loss: 0.915797\n",
            "Epoch 201\n",
            "Training   Loss: 0.778720\n",
            "Validation Loss: 0.915738\n",
            "Epoch 202\n",
            "Training   Loss: 0.776911\n",
            "Validation Loss: 0.915939\n",
            "Epoch 203\n",
            "Training   Loss: 0.775758\n",
            "Validation Loss: 0.916131\n",
            "Epoch 204\n",
            "Training   Loss: 0.773700\n",
            "Validation Loss: 0.915127\n",
            "Epoch 205\n",
            "Training   Loss: 0.772146\n",
            "Validation Loss: 0.914954\n",
            "Epoch 206\n",
            "Training   Loss: 0.770499\n",
            "Validation Loss: 0.914965\n",
            "Epoch 207\n",
            "Training   Loss: 0.770847\n",
            "Validation Loss: 0.914723\n",
            "Epoch 208\n",
            "Training   Loss: 0.768817\n",
            "Validation Loss: 0.914085\n",
            "Epoch 209\n",
            "Training   Loss: 0.767061\n",
            "Validation Loss: 0.913811\n",
            "Epoch 210\n",
            "Training   Loss: 0.763982\n",
            "Validation Loss: 0.913392\n",
            "Epoch 211\n",
            "Training   Loss: 0.763388\n",
            "Validation Loss: 0.913189\n",
            "Epoch 212\n",
            "Training   Loss: 0.761076\n",
            "Validation Loss: 0.912584\n",
            "Epoch 213\n",
            "Training   Loss: 0.760638\n",
            "Validation Loss: 0.911478\n",
            "Epoch 214\n",
            "Training   Loss: 0.758973\n",
            "Validation Loss: 0.911187\n",
            "Epoch 215\n",
            "Training   Loss: 0.756490\n",
            "Validation Loss: 0.909132\n",
            "Epoch 216\n",
            "Training   Loss: 0.755778\n",
            "Validation Loss: 0.908412\n",
            "Epoch 217\n",
            "Training   Loss: 0.753905\n",
            "Validation Loss: 0.906549\n",
            "Epoch 218\n",
            "Training   Loss: 0.752255\n",
            "Validation Loss: 0.905177\n",
            "Epoch 219\n",
            "Training   Loss: 0.751326\n",
            "Validation Loss: 0.903881\n",
            "Epoch 220\n",
            "Training   Loss: 0.750658\n",
            "Validation Loss: 0.902623\n",
            "Epoch 221\n",
            "Training   Loss: 0.747445\n",
            "Validation Loss: 0.900797\n",
            "Epoch 222\n",
            "Training   Loss: 0.746319\n",
            "Validation Loss: 0.899766\n",
            "Epoch 223\n",
            "Training   Loss: 0.744954\n",
            "Validation Loss: 0.897936\n",
            "Epoch 224\n",
            "Training   Loss: 0.744956\n",
            "Validation Loss: 0.897497\n",
            "Epoch 225\n",
            "Training   Loss: 0.743687\n",
            "Validation Loss: 0.895609\n",
            "Epoch 226\n",
            "Training   Loss: 0.742373\n",
            "Validation Loss: 0.894019\n",
            "Epoch 227\n",
            "Training   Loss: 0.741392\n",
            "Validation Loss: 0.892490\n",
            "Epoch 228\n",
            "Training   Loss: 0.738838\n",
            "Validation Loss: 0.890571\n",
            "Epoch 229\n",
            "Training   Loss: 0.737033\n",
            "Validation Loss: 0.888774\n",
            "Epoch 230\n",
            "Training   Loss: 0.735585\n",
            "Validation Loss: 0.887366\n",
            "Epoch 231\n",
            "Training   Loss: 0.734411\n",
            "Validation Loss: 0.885066\n",
            "Epoch 232\n",
            "Training   Loss: 0.732293\n",
            "Validation Loss: 0.883564\n",
            "Epoch 233\n",
            "Training   Loss: 0.730637\n",
            "Validation Loss: 0.881133\n",
            "Epoch 234\n",
            "Training   Loss: 0.728122\n",
            "Validation Loss: 0.878334\n",
            "Epoch 235\n",
            "Training   Loss: 0.726904\n",
            "Validation Loss: 0.876949\n",
            "Epoch 236\n",
            "Training   Loss: 0.725423\n",
            "Validation Loss: 0.875253\n",
            "Epoch 237\n",
            "Training   Loss: 0.723680\n",
            "Validation Loss: 0.873353\n",
            "Epoch 238\n",
            "Training   Loss: 0.722100\n",
            "Validation Loss: 0.872120\n",
            "Epoch 239\n",
            "Training   Loss: 0.720227\n",
            "Validation Loss: 0.870785\n",
            "Epoch 240\n",
            "Training   Loss: 0.718638\n",
            "Validation Loss: 0.869111\n",
            "Epoch 241\n",
            "Training   Loss: 0.719105\n",
            "Validation Loss: 0.865819\n",
            "Epoch 242\n",
            "Training   Loss: 0.719915\n",
            "Validation Loss: 0.862950\n",
            "Epoch 243\n",
            "Training   Loss: 0.718360\n",
            "Validation Loss: 0.861878\n",
            "Epoch 244\n",
            "Training   Loss: 0.716670\n",
            "Validation Loss: 0.860684\n",
            "Epoch 245\n",
            "Training   Loss: 0.715268\n",
            "Validation Loss: 0.859797\n",
            "Epoch 246\n",
            "Training   Loss: 0.713994\n",
            "Validation Loss: 0.858480\n",
            "Epoch 247\n",
            "Training   Loss: 0.712840\n",
            "Validation Loss: 0.857807\n",
            "Epoch 248\n",
            "Training   Loss: 0.711245\n",
            "Validation Loss: 0.857373\n",
            "Epoch 249\n",
            "Training   Loss: 0.709963\n",
            "Validation Loss: 0.855619\n",
            "Epoch 250\n",
            "Training   Loss: 0.708400\n",
            "Validation Loss: 0.855023\n",
            "Epoch 251\n",
            "Training   Loss: 0.707139\n",
            "Validation Loss: 0.854278\n",
            "Epoch 252\n",
            "Training   Loss: 0.706143\n",
            "Validation Loss: 0.853596\n",
            "Epoch 253\n",
            "Training   Loss: 0.704797\n",
            "Validation Loss: 0.852568\n",
            "Epoch 254\n",
            "Training   Loss: 0.703270\n",
            "Validation Loss: 0.851890\n",
            "Epoch 255\n",
            "Training   Loss: 0.702287\n",
            "Validation Loss: 0.851745\n",
            "Epoch 256\n",
            "Training   Loss: 0.701740\n",
            "Validation Loss: 0.851085\n",
            "Epoch 257\n",
            "Training   Loss: 0.701003\n",
            "Validation Loss: 0.850513\n",
            "Epoch 258\n",
            "Training   Loss: 0.700092\n",
            "Validation Loss: 0.849580\n",
            "Epoch 259\n",
            "Training   Loss: 0.699112\n",
            "Validation Loss: 0.849267\n",
            "Epoch 260\n",
            "Training   Loss: 0.697794\n",
            "Validation Loss: 0.849449\n",
            "Epoch 261\n",
            "Training   Loss: 0.697269\n",
            "Validation Loss: 0.849040\n",
            "Epoch 262\n",
            "Training   Loss: 0.696796\n",
            "Validation Loss: 0.849099\n",
            "Epoch 263\n",
            "Training   Loss: 0.695510\n",
            "Validation Loss: 0.848789\n",
            "Epoch 264\n",
            "Training   Loss: 0.694863\n",
            "Validation Loss: 0.847925\n",
            "Epoch 265\n",
            "Training   Loss: 0.694248\n",
            "Validation Loss: 0.847647\n",
            "Epoch 266\n",
            "Training   Loss: 0.693511\n",
            "Validation Loss: 0.846662\n",
            "Epoch 267\n",
            "Training   Loss: 0.692981\n",
            "Validation Loss: 0.845881\n",
            "Epoch 268\n",
            "Training   Loss: 0.692368\n",
            "Validation Loss: 0.845167\n",
            "Epoch 269\n",
            "Training   Loss: 0.691917\n",
            "Validation Loss: 0.844768\n",
            "Epoch 270\n",
            "Training   Loss: 0.691482\n",
            "Validation Loss: 0.843407\n",
            "Epoch 271\n",
            "Training   Loss: 0.691062\n",
            "Validation Loss: 0.843197\n",
            "Epoch 272\n",
            "Training   Loss: 0.690493\n",
            "Validation Loss: 0.841907\n",
            "Epoch 273\n",
            "Training   Loss: 0.690238\n",
            "Validation Loss: 0.841569\n",
            "Epoch 274\n",
            "Training   Loss: 0.689803\n",
            "Validation Loss: 0.840269\n",
            "Epoch 275\n",
            "Training   Loss: 0.689519\n",
            "Validation Loss: 0.840103\n",
            "Epoch 276\n",
            "Training   Loss: 0.688987\n",
            "Validation Loss: 0.839588\n",
            "Epoch 277\n",
            "Training   Loss: 0.688442\n",
            "Validation Loss: 0.839254\n",
            "Epoch 278\n",
            "Training   Loss: 0.688414\n",
            "Validation Loss: 0.838757\n",
            "Epoch 279\n",
            "Training   Loss: 0.688465\n",
            "Validation Loss: 0.838675\n",
            "Epoch 280\n",
            "Training   Loss: 0.687622\n",
            "Validation Loss: 0.838266\n",
            "Epoch 281\n",
            "Training   Loss: 0.687602\n",
            "Validation Loss: 0.837638\n",
            "Epoch 282\n",
            "Training   Loss: 0.687639\n",
            "Validation Loss: 0.837252\n",
            "Epoch 283\n",
            "Training   Loss: 0.687603\n",
            "Validation Loss: 0.837084\n",
            "Epoch 284\n",
            "Training   Loss: 0.687630\n",
            "Validation Loss: 0.837179\n",
            "Epoch 285\n",
            "Training   Loss: 0.687179\n",
            "Validation Loss: 0.836814\n",
            "Epoch 286\n",
            "Training   Loss: 0.686930\n",
            "Validation Loss: 0.835973\n",
            "Epoch 287\n",
            "Training   Loss: 0.687007\n",
            "Validation Loss: 0.835568\n",
            "Epoch 288\n",
            "Training   Loss: 0.686362\n",
            "Validation Loss: 0.835420\n",
            "Epoch 289\n",
            "Training   Loss: 0.685473\n",
            "Validation Loss: 0.835336\n",
            "Epoch 290\n",
            "Training   Loss: 0.685019\n",
            "Validation Loss: 0.834562\n",
            "Epoch 291\n",
            "Training   Loss: 0.684659\n",
            "Validation Loss: 0.834436\n",
            "Epoch 292\n",
            "Training   Loss: 0.683991\n",
            "Validation Loss: 0.834903\n",
            "Epoch 293\n",
            "Training   Loss: 0.683631\n",
            "Validation Loss: 0.834820\n",
            "Epoch 294\n",
            "Training   Loss: 0.683106\n",
            "Validation Loss: 0.834821\n",
            "Epoch 295\n",
            "Training   Loss: 0.682654\n",
            "Validation Loss: 0.834185\n",
            "Epoch 296\n",
            "Training   Loss: 0.682600\n",
            "Validation Loss: 0.834019\n",
            "Epoch 297\n",
            "Training   Loss: 0.682404\n",
            "Validation Loss: 0.833717\n",
            "Epoch 298\n",
            "Training   Loss: 0.682524\n",
            "Validation Loss: 0.833538\n",
            "Epoch 299\n",
            "Training   Loss: 0.682016\n",
            "Validation Loss: 0.832919\n",
            "Epoch 300\n",
            "Training   Loss: 0.681957\n",
            "Validation Loss: 0.832734\n",
            "Epoch 301\n",
            "Training   Loss: 0.682092\n",
            "Validation Loss: 0.833059\n",
            "Epoch 302\n",
            "Training   Loss: 0.682545\n",
            "Validation Loss: 0.833151\n",
            "Epoch 303\n",
            "Training   Loss: 0.682450\n",
            "Validation Loss: 0.833364\n",
            "Epoch 304\n",
            "Training   Loss: 0.682834\n",
            "Validation Loss: 0.833596\n",
            "Epoch 305\n",
            "Training   Loss: 0.682693\n",
            "Validation Loss: 0.833573\n",
            "Epoch 306\n",
            "Training   Loss: 0.683247\n",
            "Validation Loss: 0.833633\n",
            "Epoch 307\n",
            "Training   Loss: 0.683450\n",
            "Validation Loss: 0.833556\n",
            "Epoch 308\n",
            "Training   Loss: 0.683380\n",
            "Validation Loss: 0.833965\n",
            "Epoch 309\n",
            "Training   Loss: 0.683731\n",
            "Validation Loss: 0.834236\n",
            "Epoch 310\n",
            "Training   Loss: 0.684036\n",
            "Validation Loss: 0.834446\n",
            "Epoch 311\n",
            "Training   Loss: 0.684155\n",
            "Validation Loss: 0.834603\n",
            "Epoch 312\n",
            "Training   Loss: 0.684289\n",
            "Validation Loss: 0.834459\n",
            "Epoch 313\n",
            "Training   Loss: 0.684663\n",
            "Validation Loss: 0.834412\n",
            "Epoch 314\n",
            "Training   Loss: 0.684849\n",
            "Validation Loss: 0.834321\n",
            "Epoch 315\n",
            "Training   Loss: 0.685099\n",
            "Validation Loss: 0.834062\n",
            "Epoch 316\n",
            "Training   Loss: 0.685180\n",
            "Validation Loss: 0.833443\n",
            "Epoch 317\n",
            "Training   Loss: 0.684916\n",
            "Validation Loss: 0.833380\n",
            "Epoch 318\n",
            "Training   Loss: 0.685017\n",
            "Validation Loss: 0.833302\n",
            "Epoch 319\n",
            "Training   Loss: 0.685281\n",
            "Validation Loss: 0.833238\n",
            "Epoch 320\n",
            "Training   Loss: 0.685457\n",
            "Validation Loss: 0.832720\n",
            "Epoch 321\n",
            "Training   Loss: 0.685593\n",
            "Validation Loss: 0.832813\n",
            "Epoch 322\n",
            "Training   Loss: 0.685551\n",
            "Validation Loss: 0.833055\n",
            "Epoch 323\n",
            "Training   Loss: 0.685569\n",
            "Validation Loss: 0.833182\n",
            "Epoch 324\n",
            "Training   Loss: 0.685117\n",
            "Validation Loss: 0.832672\n",
            "Epoch 325\n",
            "Training   Loss: 0.684682\n",
            "Validation Loss: 0.833101\n",
            "Epoch 326\n",
            "Training   Loss: 0.684791\n",
            "Validation Loss: 0.832788\n",
            "Epoch 327\n",
            "Training   Loss: 0.684970\n",
            "Validation Loss: 0.832571\n",
            "Epoch 328\n",
            "Training   Loss: 0.685102\n",
            "Validation Loss: 0.832615\n",
            "Epoch 329\n",
            "Training   Loss: 0.685448\n",
            "Validation Loss: 0.831953\n",
            "Epoch 330\n",
            "Training   Loss: 0.685359\n",
            "Validation Loss: 0.832464\n",
            "Epoch 331\n",
            "Training   Loss: 0.685631\n",
            "Validation Loss: 0.832103\n",
            "Epoch 332\n",
            "Training   Loss: 0.685662\n",
            "Validation Loss: 0.832160\n",
            "Epoch 333\n",
            "Training   Loss: 0.686046\n",
            "Validation Loss: 0.833000\n",
            "Epoch 334\n",
            "Training   Loss: 0.685685\n",
            "Validation Loss: 0.832941\n",
            "Epoch 335\n",
            "Training   Loss: 0.685880\n",
            "Validation Loss: 0.832864\n",
            "Epoch 336\n",
            "Training   Loss: 0.685736\n",
            "Validation Loss: 0.832243\n",
            "Epoch 337\n",
            "Training   Loss: 0.685803\n",
            "Validation Loss: 0.832135\n",
            "Epoch 338\n",
            "Training   Loss: 0.685669\n",
            "Validation Loss: 0.832007\n",
            "Epoch 339\n",
            "Training   Loss: 0.685279\n",
            "Validation Loss: 0.831832\n",
            "Epoch 340\n",
            "Training   Loss: 0.684764\n",
            "Validation Loss: 0.831034\n",
            "Epoch 341\n",
            "Training   Loss: 0.684281\n",
            "Validation Loss: 0.830588\n",
            "Epoch 342\n",
            "Training   Loss: 0.683985\n",
            "Validation Loss: 0.830481\n",
            "Epoch 343\n",
            "Training   Loss: 0.683915\n",
            "Validation Loss: 0.830305\n",
            "Epoch 344\n",
            "Training   Loss: 0.683815\n",
            "Validation Loss: 0.830075\n",
            "Epoch 345\n",
            "Training   Loss: 0.683941\n",
            "Validation Loss: 0.830527\n",
            "Epoch 346\n",
            "Training   Loss: 0.684259\n",
            "Validation Loss: 0.831072\n",
            "Epoch 347\n",
            "Training   Loss: 0.684266\n",
            "Validation Loss: 0.831005\n",
            "Epoch 348\n",
            "Training   Loss: 0.684204\n",
            "Validation Loss: 0.830377\n",
            "Epoch 349\n",
            "Training   Loss: 0.684277\n",
            "Validation Loss: 0.829931\n",
            "Epoch 350\n",
            "Training   Loss: 0.684037\n",
            "Validation Loss: 0.829885\n",
            "Epoch 351\n",
            "Training   Loss: 0.684171\n",
            "Validation Loss: 0.829780\n",
            "Epoch 352\n",
            "Training   Loss: 0.684071\n",
            "Validation Loss: 0.829597\n",
            "Epoch 353\n",
            "Training   Loss: 0.683881\n",
            "Validation Loss: 0.829767\n",
            "Epoch 354\n",
            "Training   Loss: 0.683874\n",
            "Validation Loss: 0.829311\n",
            "Epoch 355\n",
            "Training   Loss: 0.683816\n",
            "Validation Loss: 0.828578\n",
            "Epoch 356\n",
            "Training   Loss: 0.683916\n",
            "Validation Loss: 0.828609\n",
            "Epoch 357\n",
            "Training   Loss: 0.683858\n",
            "Validation Loss: 0.828574\n",
            "Epoch 358\n",
            "Training   Loss: 0.683947\n",
            "Validation Loss: 0.828431\n",
            "Epoch 359\n",
            "Training   Loss: 0.683916\n",
            "Validation Loss: 0.828238\n",
            "Epoch 360\n",
            "Training   Loss: 0.684051\n",
            "Validation Loss: 0.828289\n",
            "Epoch 361\n",
            "Training   Loss: 0.683934\n",
            "Validation Loss: 0.827973\n",
            "Epoch 362\n",
            "Training   Loss: 0.684129\n",
            "Validation Loss: 0.827712\n",
            "Epoch 363\n",
            "Training   Loss: 0.684251\n",
            "Validation Loss: 0.827434\n",
            "Epoch 364\n",
            "Training   Loss: 0.684372\n",
            "Validation Loss: 0.827226\n",
            "Epoch 365\n",
            "Training   Loss: 0.684411\n",
            "Validation Loss: 0.826466\n",
            "Epoch 366\n",
            "Training   Loss: 0.684670\n",
            "Validation Loss: 0.826163\n",
            "Epoch 367\n",
            "Training   Loss: 0.684971\n",
            "Validation Loss: 0.825556\n",
            "Epoch 368\n",
            "Training   Loss: 0.685436\n",
            "Validation Loss: 0.825326\n",
            "Epoch 369\n",
            "Training   Loss: 0.685410\n",
            "Validation Loss: 0.824713\n",
            "Epoch 370\n",
            "Training   Loss: 0.685490\n",
            "Validation Loss: 0.824517\n",
            "Epoch 371\n",
            "Training   Loss: 0.685624\n",
            "Validation Loss: 0.824456\n",
            "Epoch 372\n",
            "Training   Loss: 0.685780\n",
            "Validation Loss: 0.824269\n",
            "Epoch 373\n",
            "Training   Loss: 0.685672\n",
            "Validation Loss: 0.823730\n",
            "Epoch 374\n",
            "Training   Loss: 0.685757\n",
            "Validation Loss: 0.823439\n",
            "Epoch 375\n",
            "Training   Loss: 0.685947\n",
            "Validation Loss: 0.823266\n",
            "Epoch 376\n",
            "Training   Loss: 0.685874\n",
            "Validation Loss: 0.822694\n",
            "Epoch 377\n",
            "Training   Loss: 0.685860\n",
            "Validation Loss: 0.821952\n",
            "Epoch 378\n",
            "Training   Loss: 0.685967\n",
            "Validation Loss: 0.821377\n",
            "Epoch 379\n",
            "Training   Loss: 0.686503\n",
            "Validation Loss: 0.821921\n",
            "Epoch 380\n",
            "Training   Loss: 0.686448\n",
            "Validation Loss: 0.820672\n",
            "Epoch 381\n",
            "Training   Loss: 0.686868\n",
            "Validation Loss: 0.821635\n",
            "Epoch 382\n",
            "Training   Loss: 0.686558\n",
            "Validation Loss: 0.820498\n",
            "Epoch 383\n",
            "Training   Loss: 0.686854\n",
            "Validation Loss: 0.820973\n",
            "Epoch 384\n",
            "Training   Loss: 0.686748\n",
            "Validation Loss: 0.819943\n",
            "Epoch 385\n",
            "Training   Loss: 0.686786\n",
            "Validation Loss: 0.819881\n",
            "Epoch 386\n",
            "Training   Loss: 0.686708\n",
            "Validation Loss: 0.819909\n",
            "Epoch 387\n",
            "Training   Loss: 0.686847\n",
            "Validation Loss: 0.819761\n",
            "Epoch 388\n",
            "Training   Loss: 0.686895\n",
            "Validation Loss: 0.819424\n",
            "Epoch 389\n",
            "Training   Loss: 0.687128\n",
            "Validation Loss: 0.819917\n",
            "Epoch 390\n",
            "Training   Loss: 0.687168\n",
            "Validation Loss: 0.819486\n",
            "Epoch 391\n",
            "Training   Loss: 0.687196\n",
            "Validation Loss: 0.818467\n",
            "Epoch 392\n",
            "Training   Loss: 0.687712\n",
            "Validation Loss: 0.819348\n",
            "Epoch 393\n",
            "Training   Loss: 0.687869\n",
            "Validation Loss: 0.819138\n",
            "Epoch 394\n",
            "Training   Loss: 0.687903\n",
            "Validation Loss: 0.819035\n",
            "Epoch 395\n",
            "Training   Loss: 0.688134\n",
            "Validation Loss: 0.818572\n",
            "Epoch 396\n",
            "Training   Loss: 0.687871\n",
            "Validation Loss: 0.818140\n",
            "Epoch 397\n",
            "Training   Loss: 0.688494\n",
            "Validation Loss: 0.819110\n",
            "Epoch 398\n",
            "Training   Loss: 0.688769\n",
            "Validation Loss: 0.819254\n",
            "Epoch 399\n",
            "Training   Loss: 0.688917\n",
            "Validation Loss: 0.819642\n",
            "Epoch 400\n",
            "Training   Loss: 0.688951\n",
            "Validation Loss: 0.819460\n",
            "Epoch 401\n",
            "Training   Loss: 0.688696\n",
            "Validation Loss: 0.818363\n",
            "Epoch 402\n",
            "Training   Loss: 0.688881\n",
            "Validation Loss: 0.818591\n",
            "Epoch 403\n",
            "Training   Loss: 0.688944\n",
            "Validation Loss: 0.818000\n",
            "Epoch 404\n",
            "Training   Loss: 0.688862\n",
            "Validation Loss: 0.817359\n",
            "Epoch 405\n",
            "Training   Loss: 0.688733\n",
            "Validation Loss: 0.816972\n",
            "Epoch 406\n",
            "Training   Loss: 0.688647\n",
            "Validation Loss: 0.816546\n",
            "Epoch 407\n",
            "Training   Loss: 0.688482\n",
            "Validation Loss: 0.815973\n",
            "Epoch 408\n",
            "Training   Loss: 0.688412\n",
            "Validation Loss: 0.815882\n",
            "Epoch 409\n",
            "Training   Loss: 0.688231\n",
            "Validation Loss: 0.815419\n",
            "Epoch 410\n",
            "Training   Loss: 0.688266\n",
            "Validation Loss: 0.815447\n",
            "Epoch 411\n",
            "Training   Loss: 0.688509\n",
            "Validation Loss: 0.814915\n",
            "Epoch 412\n",
            "Training   Loss: 0.688315\n",
            "Validation Loss: 0.814468\n",
            "Epoch 413\n",
            "Training   Loss: 0.689159\n",
            "Validation Loss: 0.814421\n",
            "Epoch 414\n",
            "Training   Loss: 0.689031\n",
            "Validation Loss: 0.813530\n",
            "Epoch 415\n",
            "Training   Loss: 0.688912\n",
            "Validation Loss: 0.813018\n",
            "Epoch 416\n",
            "Training   Loss: 0.688722\n",
            "Validation Loss: 0.812993\n",
            "Epoch 417\n",
            "Training   Loss: 0.688592\n",
            "Validation Loss: 0.812721\n",
            "Epoch 418\n",
            "Training   Loss: 0.688322\n",
            "Validation Loss: 0.812065\n",
            "Epoch 419\n",
            "Training   Loss: 0.687651\n",
            "Validation Loss: 0.811094\n",
            "Epoch 420\n",
            "Training   Loss: 0.687390\n",
            "Validation Loss: 0.810496\n",
            "Epoch 421\n",
            "Training   Loss: 0.687220\n",
            "Validation Loss: 0.810114\n",
            "Epoch 422\n",
            "Training   Loss: 0.687056\n",
            "Validation Loss: 0.809688\n",
            "Epoch 423\n",
            "Training   Loss: 0.687223\n",
            "Validation Loss: 0.809555\n",
            "Epoch 424\n",
            "Training   Loss: 0.687186\n",
            "Validation Loss: 0.809284\n",
            "Epoch 425\n",
            "Training   Loss: 0.686687\n",
            "Validation Loss: 0.808608\n",
            "Epoch 426\n",
            "Training   Loss: 0.686574\n",
            "Validation Loss: 0.808195\n",
            "Epoch 427\n",
            "Training   Loss: 0.686186\n",
            "Validation Loss: 0.807861\n",
            "Epoch 428\n",
            "Training   Loss: 0.685929\n",
            "Validation Loss: 0.807558\n",
            "Epoch 429\n",
            "Training   Loss: 0.686032\n",
            "Validation Loss: 0.807786\n",
            "Epoch 430\n",
            "Training   Loss: 0.686225\n",
            "Validation Loss: 0.807740\n",
            "Epoch 431\n",
            "Training   Loss: 0.685823\n",
            "Validation Loss: 0.807341\n",
            "Epoch 432\n",
            "Training   Loss: 0.685321\n",
            "Validation Loss: 0.806763\n",
            "Epoch 433\n",
            "Training   Loss: 0.685516\n",
            "Validation Loss: 0.807106\n",
            "Epoch 434\n",
            "Training   Loss: 0.685414\n",
            "Validation Loss: 0.806805\n",
            "Epoch 435\n",
            "Training   Loss: 0.685307\n",
            "Validation Loss: 0.806130\n",
            "Epoch 436\n",
            "Training   Loss: 0.685187\n",
            "Validation Loss: 0.805919\n",
            "Epoch 437\n",
            "Training   Loss: 0.685228\n",
            "Validation Loss: 0.805675\n",
            "Epoch 438\n",
            "Training   Loss: 0.685128\n",
            "Validation Loss: 0.805173\n",
            "Epoch 439\n",
            "Training   Loss: 0.685031\n",
            "Validation Loss: 0.804853\n",
            "Epoch 440\n",
            "Training   Loss: 0.684876\n",
            "Validation Loss: 0.804842\n",
            "Epoch 441\n",
            "Training   Loss: 0.684717\n",
            "Validation Loss: 0.804754\n",
            "Epoch 442\n",
            "Training   Loss: 0.684361\n",
            "Validation Loss: 0.804296\n",
            "Epoch 443\n",
            "Training   Loss: 0.684012\n",
            "Validation Loss: 0.804474\n",
            "Epoch 444\n",
            "Training   Loss: 0.683887\n",
            "Validation Loss: 0.803866\n",
            "Epoch 445\n",
            "Training   Loss: 0.683673\n",
            "Validation Loss: 0.803608\n",
            "Epoch 446\n",
            "Training   Loss: 0.683465\n",
            "Validation Loss: 0.802829\n",
            "Epoch 447\n",
            "Training   Loss: 0.682999\n",
            "Validation Loss: 0.802654\n",
            "Epoch 448\n",
            "Training   Loss: 0.682605\n",
            "Validation Loss: 0.802528\n",
            "Epoch 449\n",
            "Training   Loss: 0.682533\n",
            "Validation Loss: 0.802430\n",
            "Epoch 450\n",
            "Training   Loss: 0.682137\n",
            "Validation Loss: 0.802068\n",
            "Epoch 451\n",
            "Training   Loss: 0.681780\n",
            "Validation Loss: 0.801625\n",
            "Epoch 452\n",
            "Training   Loss: 0.681637\n",
            "Validation Loss: 0.801340\n",
            "Epoch 453\n",
            "Training   Loss: 0.681434\n",
            "Validation Loss: 0.800958\n",
            "Epoch 454\n",
            "Training   Loss: 0.681134\n",
            "Validation Loss: 0.800595\n",
            "Epoch 455\n",
            "Training   Loss: 0.680903\n",
            "Validation Loss: 0.800129\n",
            "Epoch 456\n",
            "Training   Loss: 0.680400\n",
            "Validation Loss: 0.799697\n",
            "Epoch 457\n",
            "Training   Loss: 0.679979\n",
            "Validation Loss: 0.798846\n",
            "Epoch 458\n",
            "Training   Loss: 0.679881\n",
            "Validation Loss: 0.798274\n",
            "Epoch 459\n",
            "Training   Loss: 0.679831\n",
            "Validation Loss: 0.798232\n",
            "Epoch 460\n",
            "Training   Loss: 0.679420\n",
            "Validation Loss: 0.797817\n",
            "Epoch 461\n",
            "Training   Loss: 0.679160\n",
            "Validation Loss: 0.797598\n",
            "Epoch 462\n",
            "Training   Loss: 0.678942\n",
            "Validation Loss: 0.797353\n",
            "Epoch 463\n",
            "Training   Loss: 0.678840\n",
            "Validation Loss: 0.797214\n",
            "Epoch 464\n",
            "Training   Loss: 0.678596\n",
            "Validation Loss: 0.797017\n",
            "Epoch 465\n",
            "Training   Loss: 0.678745\n",
            "Validation Loss: 0.796962\n",
            "Epoch 466\n",
            "Training   Loss: 0.678468\n",
            "Validation Loss: 0.796676\n",
            "Epoch 467\n",
            "Training   Loss: 0.678361\n",
            "Validation Loss: 0.796534\n",
            "Epoch 468\n",
            "Training   Loss: 0.678262\n",
            "Validation Loss: 0.796287\n",
            "Epoch 469\n",
            "Training   Loss: 0.678167\n",
            "Validation Loss: 0.796195\n",
            "Epoch 470\n",
            "Training   Loss: 0.678013\n",
            "Validation Loss: 0.795817\n",
            "Epoch 471\n",
            "Training   Loss: 0.677686\n",
            "Validation Loss: 0.795377\n",
            "Epoch 472\n",
            "Training   Loss: 0.677584\n",
            "Validation Loss: 0.795345\n",
            "Epoch 473\n",
            "Training   Loss: 0.677400\n",
            "Validation Loss: 0.795480\n",
            "Epoch 474\n",
            "Training   Loss: 0.676841\n",
            "Validation Loss: 0.794865\n",
            "Epoch 475\n",
            "Training   Loss: 0.676897\n",
            "Validation Loss: 0.794464\n",
            "Epoch 476\n",
            "Training   Loss: 0.676825\n",
            "Validation Loss: 0.794064\n",
            "Epoch 477\n",
            "Training   Loss: 0.676451\n",
            "Validation Loss: 0.793253\n",
            "Epoch 478\n",
            "Training   Loss: 0.675965\n",
            "Validation Loss: 0.793023\n",
            "Epoch 479\n",
            "Training   Loss: 0.675805\n",
            "Validation Loss: 0.792498\n",
            "Epoch 480\n",
            "Training   Loss: 0.675641\n",
            "Validation Loss: 0.792728\n",
            "Epoch 481\n",
            "Training   Loss: 0.675253\n",
            "Validation Loss: 0.792349\n",
            "Epoch 482\n",
            "Training   Loss: 0.675219\n",
            "Validation Loss: 0.792143\n",
            "Epoch 483\n",
            "Training   Loss: 0.674606\n",
            "Validation Loss: 0.792364\n",
            "Epoch 484\n",
            "Training   Loss: 0.674597\n",
            "Validation Loss: 0.792075\n",
            "Epoch 485\n",
            "Training   Loss: 0.674353\n",
            "Validation Loss: 0.791928\n",
            "Epoch 486\n",
            "Training   Loss: 0.674176\n",
            "Validation Loss: 0.791264\n",
            "Epoch 487\n",
            "Training   Loss: 0.674551\n",
            "Validation Loss: 0.792195\n",
            "Epoch 488\n",
            "Training   Loss: 0.674391\n",
            "Validation Loss: 0.791451\n",
            "Epoch 489\n",
            "Training   Loss: 0.674358\n",
            "Validation Loss: 0.791754\n",
            "Epoch 490\n",
            "Training   Loss: 0.674284\n",
            "Validation Loss: 0.791246\n",
            "Epoch 491\n",
            "Training   Loss: 0.674105\n",
            "Validation Loss: 0.791394\n",
            "Epoch 492\n",
            "Training   Loss: 0.674051\n",
            "Validation Loss: 0.791234\n",
            "Epoch 493\n",
            "Training   Loss: 0.673930\n",
            "Validation Loss: 0.791058\n",
            "Epoch 494\n",
            "Training   Loss: 0.673879\n",
            "Validation Loss: 0.791071\n",
            "Epoch 495\n",
            "Training   Loss: 0.673712\n",
            "Validation Loss: 0.790800\n",
            "Epoch 496\n",
            "Training   Loss: 0.673553\n",
            "Validation Loss: 0.790943\n",
            "Epoch 497\n",
            "Training   Loss: 0.673648\n",
            "Validation Loss: 0.790614\n",
            "Epoch 498\n",
            "Training   Loss: 0.673364\n",
            "Validation Loss: 0.790128\n",
            "Epoch 499\n",
            "Training   Loss: 0.672984\n",
            "Validation Loss: 0.789497\n",
            "Epoch 500\n",
            "Training   Loss: 0.672875\n",
            "Validation Loss: 0.789033\n",
            "Epoch 501\n",
            "Training   Loss: 0.672509\n",
            "Validation Loss: 0.788877\n",
            "Epoch 502\n",
            "Training   Loss: 0.672661\n",
            "Validation Loss: 0.788559\n",
            "Epoch 503\n",
            "Training   Loss: 0.672561\n",
            "Validation Loss: 0.788206\n",
            "Epoch 504\n",
            "Training   Loss: 0.672684\n",
            "Validation Loss: 0.787556\n",
            "Epoch 505\n",
            "Training   Loss: 0.672449\n",
            "Validation Loss: 0.787398\n",
            "Epoch 506\n",
            "Training   Loss: 0.672387\n",
            "Validation Loss: 0.786623\n",
            "Epoch 507\n",
            "Training   Loss: 0.672603\n",
            "Validation Loss: 0.787438\n",
            "Epoch 508\n",
            "Training   Loss: 0.672561\n",
            "Validation Loss: 0.787167\n",
            "Epoch 509\n",
            "Training   Loss: 0.672487\n",
            "Validation Loss: 0.786526\n",
            "Epoch 510\n",
            "Training   Loss: 0.672755\n",
            "Validation Loss: 0.786027\n",
            "Epoch 511\n",
            "Training   Loss: 0.672617\n",
            "Validation Loss: 0.785661\n",
            "Epoch 512\n",
            "Training   Loss: 0.672759\n",
            "Validation Loss: 0.785735\n",
            "Epoch 513\n",
            "Training   Loss: 0.672991\n",
            "Validation Loss: 0.785667\n",
            "Epoch 514\n",
            "Training   Loss: 0.672918\n",
            "Validation Loss: 0.785663\n",
            "Epoch 515\n",
            "Training   Loss: 0.673007\n",
            "Validation Loss: 0.785783\n",
            "Epoch 516\n",
            "Training   Loss: 0.672496\n",
            "Validation Loss: 0.784722\n",
            "Epoch 517\n",
            "Training   Loss: 0.672568\n",
            "Validation Loss: 0.784581\n",
            "Epoch 518\n",
            "Training   Loss: 0.672542\n",
            "Validation Loss: 0.784406\n",
            "Epoch 519\n",
            "Training   Loss: 0.672453\n",
            "Validation Loss: 0.784013\n",
            "Epoch 520\n",
            "Training   Loss: 0.672473\n",
            "Validation Loss: 0.783912\n",
            "Epoch 521\n",
            "Training   Loss: 0.672449\n",
            "Validation Loss: 0.783868\n",
            "Epoch 522\n",
            "Training   Loss: 0.672312\n",
            "Validation Loss: 0.783433\n",
            "Epoch 523\n",
            "Training   Loss: 0.672378\n",
            "Validation Loss: 0.783363\n",
            "Epoch 524\n",
            "Training   Loss: 0.672682\n",
            "Validation Loss: 0.782651\n",
            "Epoch 525\n",
            "Training   Loss: 0.672475\n",
            "Validation Loss: 0.781671\n",
            "Epoch 526\n",
            "Training   Loss: 0.672504\n",
            "Validation Loss: 0.781711\n",
            "Epoch 527\n",
            "Training   Loss: 0.672317\n",
            "Validation Loss: 0.781550\n",
            "Epoch 528\n",
            "Training   Loss: 0.671766\n",
            "Validation Loss: 0.781378\n",
            "Epoch 529\n",
            "Training   Loss: 0.671727\n",
            "Validation Loss: 0.781148\n",
            "Epoch 530\n",
            "Training   Loss: 0.671493\n",
            "Validation Loss: 0.781554\n",
            "Epoch 531\n",
            "Training   Loss: 0.671055\n",
            "Validation Loss: 0.781364\n",
            "Epoch 532\n",
            "Training   Loss: 0.670781\n",
            "Validation Loss: 0.781371\n",
            "Epoch 533\n",
            "Training   Loss: 0.670332\n",
            "Validation Loss: 0.781158\n",
            "Epoch 534\n",
            "Training   Loss: 0.670154\n",
            "Validation Loss: 0.780940\n",
            "Epoch 535\n",
            "Training   Loss: 0.670084\n",
            "Validation Loss: 0.780853\n",
            "Epoch 536\n",
            "Training   Loss: 0.669668\n",
            "Validation Loss: 0.780228\n",
            "Epoch 537\n",
            "Training   Loss: 0.669422\n",
            "Validation Loss: 0.780607\n",
            "Epoch 538\n",
            "Training   Loss: 0.669135\n",
            "Validation Loss: 0.780976\n",
            "Epoch 539\n",
            "Training   Loss: 0.668871\n",
            "Validation Loss: 0.780885\n",
            "Epoch 540\n",
            "Training   Loss: 0.668701\n",
            "Validation Loss: 0.781082\n",
            "Epoch 541\n",
            "Training   Loss: 0.668252\n",
            "Validation Loss: 0.779925\n",
            "Epoch 542\n",
            "Training   Loss: 0.668004\n",
            "Validation Loss: 0.780000\n",
            "Epoch 543\n",
            "Training   Loss: 0.667604\n",
            "Validation Loss: 0.779961\n",
            "Epoch 544\n",
            "Training   Loss: 0.667179\n",
            "Validation Loss: 0.779681\n",
            "Epoch 545\n",
            "Training   Loss: 0.667163\n",
            "Validation Loss: 0.779748\n",
            "Epoch 546\n",
            "Training   Loss: 0.666777\n",
            "Validation Loss: 0.779493\n",
            "Epoch 547\n",
            "Training   Loss: 0.666622\n",
            "Validation Loss: 0.779457\n",
            "Epoch 548\n",
            "Training   Loss: 0.666296\n",
            "Validation Loss: 0.779449\n",
            "Epoch 549\n",
            "Training   Loss: 0.665965\n",
            "Validation Loss: 0.779098\n",
            "Epoch 550\n",
            "Training   Loss: 0.665677\n",
            "Validation Loss: 0.779584\n",
            "Epoch 551\n",
            "Training   Loss: 0.665440\n",
            "Validation Loss: 0.779063\n",
            "Epoch 552\n",
            "Training   Loss: 0.665803\n",
            "Validation Loss: 0.779333\n",
            "Epoch 553\n",
            "Training   Loss: 0.665636\n",
            "Validation Loss: 0.779444\n",
            "Epoch 554\n",
            "Training   Loss: 0.665316\n",
            "Validation Loss: 0.779457\n",
            "Epoch 555\n",
            "Training   Loss: 0.665215\n",
            "Validation Loss: 0.779377\n",
            "Epoch 556\n",
            "Training   Loss: 0.665212\n",
            "Validation Loss: 0.779276\n",
            "Epoch 557\n",
            "Training   Loss: 0.664667\n",
            "Validation Loss: 0.778845\n",
            "Epoch 558\n",
            "Training   Loss: 0.664706\n",
            "Validation Loss: 0.779404\n",
            "Epoch 559\n",
            "Training   Loss: 0.664483\n",
            "Validation Loss: 0.779243\n",
            "Epoch 560\n",
            "Training   Loss: 0.664404\n",
            "Validation Loss: 0.779052\n",
            "Epoch 561\n",
            "Training   Loss: 0.664151\n",
            "Validation Loss: 0.778814\n",
            "Epoch 562\n",
            "Training   Loss: 0.663933\n",
            "Validation Loss: 0.778641\n",
            "Epoch 563\n",
            "Training   Loss: 0.663988\n",
            "Validation Loss: 0.778583\n",
            "Epoch 564\n",
            "Training   Loss: 0.663967\n",
            "Validation Loss: 0.778903\n",
            "Epoch 565\n",
            "Training   Loss: 0.663677\n",
            "Validation Loss: 0.778548\n",
            "Epoch 566\n",
            "Training   Loss: 0.663376\n",
            "Validation Loss: 0.778667\n",
            "Epoch 567\n",
            "Training   Loss: 0.663530\n",
            "Validation Loss: 0.779085\n",
            "Epoch 568\n",
            "Training   Loss: 0.663297\n",
            "Validation Loss: 0.778913\n",
            "Epoch 569\n",
            "Training   Loss: 0.663328\n",
            "Validation Loss: 0.779360\n",
            "Epoch 570\n",
            "Training   Loss: 0.663070\n",
            "Validation Loss: 0.779284\n",
            "Epoch 571\n",
            "Training   Loss: 0.663217\n",
            "Validation Loss: 0.779481\n",
            "Epoch 572\n",
            "Training   Loss: 0.663095\n",
            "Validation Loss: 0.779490\n",
            "Epoch 573\n",
            "Training   Loss: 0.662836\n",
            "Validation Loss: 0.779585\n",
            "Epoch 574\n",
            "Training   Loss: 0.662872\n",
            "Validation Loss: 0.779700\n",
            "Epoch 575\n",
            "Training   Loss: 0.662920\n",
            "Validation Loss: 0.779614\n",
            "Epoch 576\n",
            "Training   Loss: 0.663520\n",
            "Validation Loss: 0.779826\n",
            "Epoch 577\n",
            "Training   Loss: 0.663487\n",
            "Validation Loss: 0.780220\n",
            "Epoch 578\n",
            "Training   Loss: 0.663512\n",
            "Validation Loss: 0.779943\n",
            "Epoch 579\n",
            "Training   Loss: 0.663091\n",
            "Validation Loss: 0.779886\n",
            "Epoch 580\n",
            "Training   Loss: 0.662748\n",
            "Validation Loss: 0.779595\n",
            "Epoch 581\n",
            "Training   Loss: 0.662485\n",
            "Validation Loss: 0.779031\n",
            "Epoch 582\n",
            "Training   Loss: 0.662256\n",
            "Validation Loss: 0.778959\n",
            "Epoch 583\n",
            "Training   Loss: 0.662320\n",
            "Validation Loss: 0.779179\n",
            "Epoch 584\n",
            "Training   Loss: 0.662078\n",
            "Validation Loss: 0.778960\n",
            "Epoch 585\n",
            "Training   Loss: 0.662208\n",
            "Validation Loss: 0.779385\n",
            "Epoch 586\n",
            "Training   Loss: 0.662015\n",
            "Validation Loss: 0.779152\n",
            "Epoch 587\n",
            "Training   Loss: 0.661894\n",
            "Validation Loss: 0.779027\n",
            "Epoch 588\n",
            "Training   Loss: 0.661977\n",
            "Validation Loss: 0.779021\n",
            "Epoch 589\n",
            "Training   Loss: 0.661927\n",
            "Validation Loss: 0.778708\n",
            "Epoch 590\n",
            "Training   Loss: 0.661731\n",
            "Validation Loss: 0.779084\n",
            "Epoch 591\n",
            "Training   Loss: 0.661673\n",
            "Validation Loss: 0.778707\n",
            "Epoch 592\n",
            "Training   Loss: 0.661640\n",
            "Validation Loss: 0.778520\n",
            "Epoch 593\n",
            "Training   Loss: 0.661443\n",
            "Validation Loss: 0.778312\n",
            "Epoch 594\n",
            "Training   Loss: 0.662013\n",
            "Validation Loss: 0.778958\n",
            "Epoch 595\n",
            "Training   Loss: 0.661893\n",
            "Validation Loss: 0.778971\n",
            "Epoch 596\n",
            "Training   Loss: 0.661819\n",
            "Validation Loss: 0.779288\n",
            "Epoch 597\n",
            "Training   Loss: 0.661532\n",
            "Validation Loss: 0.779327\n",
            "Epoch 598\n",
            "Training   Loss: 0.661792\n",
            "Validation Loss: 0.779250\n",
            "Epoch 599\n",
            "Training   Loss: 0.662048\n",
            "Validation Loss: 0.779499\n",
            "Epoch 600\n",
            "Training   Loss: 0.662022\n",
            "Validation Loss: 0.779657\n",
            "Epoch 601\n",
            "Training   Loss: 0.662025\n",
            "Validation Loss: 0.779595\n",
            "Epoch 602\n",
            "Training   Loss: 0.661612\n",
            "Validation Loss: 0.779410\n",
            "Epoch 603\n",
            "Training   Loss: 0.661620\n",
            "Validation Loss: 0.779406\n",
            "Epoch 604\n",
            "Training   Loss: 0.661430\n",
            "Validation Loss: 0.779632\n",
            "Epoch 605\n",
            "Training   Loss: 0.661745\n",
            "Validation Loss: 0.779765\n",
            "Epoch 606\n",
            "Training   Loss: 0.661661\n",
            "Validation Loss: 0.779719\n",
            "Epoch 607\n",
            "Training   Loss: 0.661645\n",
            "Validation Loss: 0.779830\n",
            "Epoch 608\n",
            "Training   Loss: 0.661983\n",
            "Validation Loss: 0.779796\n",
            "Epoch 609\n",
            "Training   Loss: 0.661558\n",
            "Validation Loss: 0.779568\n",
            "Epoch 610\n",
            "Training   Loss: 0.661743\n",
            "Validation Loss: 0.779823\n",
            "Epoch 611\n",
            "Training   Loss: 0.661456\n",
            "Validation Loss: 0.779238\n",
            "Epoch 612\n",
            "Training   Loss: 0.661401\n",
            "Validation Loss: 0.779243\n",
            "Epoch 613\n",
            "Training   Loss: 0.661501\n",
            "Validation Loss: 0.779537\n",
            "Epoch 614\n",
            "Training   Loss: 0.661000\n",
            "Validation Loss: 0.779208\n",
            "Epoch 615\n",
            "Training   Loss: 0.661137\n",
            "Validation Loss: 0.779333\n",
            "Epoch 616\n",
            "Training   Loss: 0.660982\n",
            "Validation Loss: 0.779569\n",
            "Epoch 617\n",
            "Training   Loss: 0.661076\n",
            "Validation Loss: 0.779630\n",
            "Epoch 618\n",
            "Training   Loss: 0.660595\n",
            "Validation Loss: 0.779219\n",
            "Epoch 619\n",
            "Training   Loss: 0.660650\n",
            "Validation Loss: 0.779125\n",
            "Epoch 620\n",
            "Training   Loss: 0.660528\n",
            "Validation Loss: 0.779084\n",
            "Epoch 621\n",
            "Training   Loss: 0.660561\n",
            "Validation Loss: 0.778972\n",
            "Epoch 622\n",
            "Training   Loss: 0.660172\n",
            "Validation Loss: 0.778800\n",
            "Epoch 623\n",
            "Training   Loss: 0.660141\n",
            "Validation Loss: 0.778810\n",
            "Epoch 624\n",
            "Training   Loss: 0.659987\n",
            "Validation Loss: 0.778383\n",
            "Epoch 625\n",
            "Training   Loss: 0.659898\n",
            "Validation Loss: 0.778031\n",
            "Epoch 626\n",
            "Training   Loss: 0.659800\n",
            "Validation Loss: 0.778033\n",
            "Epoch 627\n",
            "Training   Loss: 0.660305\n",
            "Validation Loss: 0.778005\n",
            "Epoch 628\n",
            "Training   Loss: 0.660126\n",
            "Validation Loss: 0.777882\n",
            "Epoch 629\n",
            "Training   Loss: 0.660315\n",
            "Validation Loss: 0.777890\n",
            "Epoch 630\n",
            "Training   Loss: 0.659951\n",
            "Validation Loss: 0.777715\n",
            "Epoch 631\n",
            "Training   Loss: 0.659339\n",
            "Validation Loss: 0.776960\n",
            "Epoch 632\n",
            "Training   Loss: 0.659574\n",
            "Validation Loss: 0.777090\n",
            "Epoch 633\n",
            "Training   Loss: 0.659352\n",
            "Validation Loss: 0.776885\n",
            "Epoch 634\n",
            "Training   Loss: 0.659394\n",
            "Validation Loss: 0.776993\n",
            "Epoch 635\n",
            "Training   Loss: 0.659253\n",
            "Validation Loss: 0.777002\n",
            "Epoch 636\n",
            "Training   Loss: 0.659239\n",
            "Validation Loss: 0.776860\n",
            "Epoch 637\n",
            "Training   Loss: 0.659041\n",
            "Validation Loss: 0.776692\n",
            "Epoch 638\n",
            "Training   Loss: 0.659051\n",
            "Validation Loss: 0.776693\n",
            "Epoch 639\n",
            "Training   Loss: 0.659049\n",
            "Validation Loss: 0.776715\n",
            "Epoch 640\n",
            "Training   Loss: 0.658844\n",
            "Validation Loss: 0.776795\n",
            "Epoch 641\n",
            "Training   Loss: 0.658455\n",
            "Validation Loss: 0.776619\n",
            "Epoch 642\n",
            "Training   Loss: 0.658807\n",
            "Validation Loss: 0.777057\n",
            "Epoch 643\n",
            "Training   Loss: 0.658842\n",
            "Validation Loss: 0.777029\n",
            "Epoch 644\n",
            "Training   Loss: 0.658812\n",
            "Validation Loss: 0.777035\n",
            "Epoch 645\n",
            "Training   Loss: 0.658446\n",
            "Validation Loss: 0.776604\n",
            "Epoch 646\n",
            "Training   Loss: 0.658527\n",
            "Validation Loss: 0.776408\n",
            "Epoch 647\n",
            "Training   Loss: 0.658620\n",
            "Validation Loss: 0.776652\n",
            "Epoch 648\n",
            "Training   Loss: 0.658643\n",
            "Validation Loss: 0.776517\n",
            "Epoch 649\n",
            "Training   Loss: 0.658625\n",
            "Validation Loss: 0.776545\n",
            "Epoch 650\n",
            "Training   Loss: 0.658527\n",
            "Validation Loss: 0.776398\n",
            "Epoch 651\n",
            "Training   Loss: 0.658607\n",
            "Validation Loss: 0.776319\n",
            "Epoch 652\n",
            "Training   Loss: 0.658503\n",
            "Validation Loss: 0.776132\n",
            "Epoch 653\n",
            "Training   Loss: 0.658385\n",
            "Validation Loss: 0.775649\n",
            "Epoch 654\n",
            "Training   Loss: 0.658503\n",
            "Validation Loss: 0.775439\n",
            "Epoch 655\n",
            "Training   Loss: 0.658596\n",
            "Validation Loss: 0.775277\n",
            "Epoch 656\n",
            "Training   Loss: 0.658600\n",
            "Validation Loss: 0.775177\n",
            "Epoch 657\n",
            "Training   Loss: 0.658750\n",
            "Validation Loss: 0.775528\n",
            "Epoch 658\n",
            "Training   Loss: 0.658497\n",
            "Validation Loss: 0.775185\n",
            "Epoch 659\n",
            "Training   Loss: 0.658349\n",
            "Validation Loss: 0.775116\n",
            "Epoch 660\n",
            "Training   Loss: 0.658434\n",
            "Validation Loss: 0.775220\n",
            "Epoch 661\n",
            "Training   Loss: 0.658242\n",
            "Validation Loss: 0.774687\n",
            "Epoch 662\n",
            "Training   Loss: 0.658090\n",
            "Validation Loss: 0.774694\n",
            "Epoch 663\n",
            "Training   Loss: 0.657868\n",
            "Validation Loss: 0.774277\n",
            "Epoch 664\n",
            "Training   Loss: 0.657747\n",
            "Validation Loss: 0.774041\n",
            "Epoch 665\n",
            "Training   Loss: 0.657650\n",
            "Validation Loss: 0.774027\n",
            "Epoch 666\n",
            "Training   Loss: 0.657628\n",
            "Validation Loss: 0.773922\n",
            "Epoch 667\n",
            "Training   Loss: 0.657171\n",
            "Validation Loss: 0.773164\n",
            "Epoch 668\n",
            "Training   Loss: 0.657269\n",
            "Validation Loss: 0.773143\n",
            "Epoch 669\n",
            "Training   Loss: 0.657454\n",
            "Validation Loss: 0.773237\n",
            "Epoch 670\n",
            "Training   Loss: 0.657238\n",
            "Validation Loss: 0.772942\n",
            "Epoch 671\n",
            "Training   Loss: 0.657374\n",
            "Validation Loss: 0.772587\n",
            "Epoch 672\n",
            "Training   Loss: 0.657405\n",
            "Validation Loss: 0.772864\n",
            "Epoch 673\n",
            "Training   Loss: 0.657266\n",
            "Validation Loss: 0.772442\n",
            "Epoch 674\n",
            "Training   Loss: 0.657511\n",
            "Validation Loss: 0.772933\n",
            "Epoch 675\n",
            "Training   Loss: 0.657275\n",
            "Validation Loss: 0.772477\n",
            "Epoch 676\n",
            "Training   Loss: 0.656890\n",
            "Validation Loss: 0.772267\n",
            "Epoch 677\n",
            "Training   Loss: 0.656985\n",
            "Validation Loss: 0.772496\n",
            "Epoch 678\n",
            "Training   Loss: 0.656731\n",
            "Validation Loss: 0.772135\n",
            "Epoch 679\n",
            "Training   Loss: 0.656850\n",
            "Validation Loss: 0.772385\n",
            "Epoch 680\n",
            "Training   Loss: 0.656621\n",
            "Validation Loss: 0.771886\n",
            "Epoch 681\n",
            "Training   Loss: 0.656763\n",
            "Validation Loss: 0.772099\n",
            "Epoch 682\n",
            "Training   Loss: 0.657038\n",
            "Validation Loss: 0.772257\n",
            "Epoch 683\n",
            "Training   Loss: 0.657115\n",
            "Validation Loss: 0.772124\n",
            "Epoch 684\n",
            "Training   Loss: 0.657118\n",
            "Validation Loss: 0.771901\n",
            "Epoch 685\n",
            "Training   Loss: 0.657223\n",
            "Validation Loss: 0.771934\n",
            "Epoch 686\n",
            "Training   Loss: 0.656855\n",
            "Validation Loss: 0.771542\n",
            "Epoch 687\n",
            "Training   Loss: 0.656861\n",
            "Validation Loss: 0.771491\n",
            "Epoch 688\n",
            "Training   Loss: 0.656925\n",
            "Validation Loss: 0.771537\n",
            "Epoch 689\n",
            "Training   Loss: 0.657174\n",
            "Validation Loss: 0.771860\n",
            "Epoch 690\n",
            "Training   Loss: 0.656904\n",
            "Validation Loss: 0.771275\n",
            "Epoch 691\n",
            "Training   Loss: 0.657281\n",
            "Validation Loss: 0.771550\n",
            "Epoch 692\n",
            "Training   Loss: 0.657206\n",
            "Validation Loss: 0.771470\n",
            "Epoch 693\n",
            "Training   Loss: 0.657063\n",
            "Validation Loss: 0.771073\n",
            "Epoch 694\n",
            "Training   Loss: 0.656986\n",
            "Validation Loss: 0.770913\n",
            "Epoch 695\n",
            "Training   Loss: 0.657093\n",
            "Validation Loss: 0.771136\n",
            "Epoch 696\n",
            "Training   Loss: 0.656961\n",
            "Validation Loss: 0.770992\n",
            "Epoch 697\n",
            "Training   Loss: 0.656834\n",
            "Validation Loss: 0.770698\n",
            "Epoch 698\n",
            "Training   Loss: 0.656849\n",
            "Validation Loss: 0.770793\n",
            "Epoch 699\n",
            "Training   Loss: 0.656760\n",
            "Validation Loss: 0.770765\n",
            "Epoch 700\n",
            "Training   Loss: 0.656999\n",
            "Validation Loss: 0.770623\n",
            "Epoch 701\n",
            "Training   Loss: 0.656632\n",
            "Validation Loss: 0.770525\n",
            "Epoch 702\n",
            "Training   Loss: 0.656887\n",
            "Validation Loss: 0.770379\n",
            "Epoch 703\n",
            "Training   Loss: 0.656870\n",
            "Validation Loss: 0.770236\n",
            "Epoch 704\n",
            "Training   Loss: 0.656733\n",
            "Validation Loss: 0.770087\n",
            "Epoch 705\n",
            "Training   Loss: 0.656775\n",
            "Validation Loss: 0.770238\n",
            "Epoch 706\n",
            "Training   Loss: 0.656870\n",
            "Validation Loss: 0.769903\n",
            "Epoch 707\n",
            "Training   Loss: 0.656974\n",
            "Validation Loss: 0.769959\n",
            "Epoch 708\n",
            "Training   Loss: 0.656790\n",
            "Validation Loss: 0.769623\n",
            "Epoch 709\n",
            "Training   Loss: 0.656763\n",
            "Validation Loss: 0.769614\n",
            "Epoch 710\n",
            "Training   Loss: 0.656891\n",
            "Validation Loss: 0.769389\n",
            "Epoch 711\n",
            "Training   Loss: 0.657001\n",
            "Validation Loss: 0.769549\n",
            "Epoch 712\n",
            "Training   Loss: 0.657013\n",
            "Validation Loss: 0.769460\n",
            "Epoch 713\n",
            "Training   Loss: 0.656634\n",
            "Validation Loss: 0.769109\n",
            "Epoch 714\n",
            "Training   Loss: 0.656748\n",
            "Validation Loss: 0.769102\n",
            "Epoch 715\n",
            "Training   Loss: 0.657011\n",
            "Validation Loss: 0.769146\n",
            "Epoch 716\n",
            "Training   Loss: 0.657055\n",
            "Validation Loss: 0.769071\n",
            "Epoch 717\n",
            "Training   Loss: 0.657031\n",
            "Validation Loss: 0.768873\n",
            "Epoch 718\n",
            "Training   Loss: 0.657177\n",
            "Validation Loss: 0.768943\n",
            "Epoch 719\n",
            "Training   Loss: 0.657062\n",
            "Validation Loss: 0.768755\n",
            "Epoch 720\n",
            "Training   Loss: 0.657185\n",
            "Validation Loss: 0.768770\n",
            "Epoch 721\n",
            "Training   Loss: 0.657057\n",
            "Validation Loss: 0.768476\n",
            "Epoch 722\n",
            "Training   Loss: 0.657249\n",
            "Validation Loss: 0.768423\n",
            "Epoch 723\n",
            "Training   Loss: 0.657534\n",
            "Validation Loss: 0.768489\n",
            "Epoch 724\n",
            "Training   Loss: 0.657240\n",
            "Validation Loss: 0.767962\n",
            "Epoch 725\n",
            "Training   Loss: 0.657202\n",
            "Validation Loss: 0.767784\n",
            "Epoch 726\n",
            "Training   Loss: 0.657352\n",
            "Validation Loss: 0.767965\n",
            "Epoch 727\n",
            "Training   Loss: 0.657415\n",
            "Validation Loss: 0.768073\n",
            "Epoch 728\n",
            "Training   Loss: 0.657336\n",
            "Validation Loss: 0.767817\n",
            "Epoch 729\n",
            "Training   Loss: 0.657505\n",
            "Validation Loss: 0.767783\n",
            "Epoch 730\n",
            "Training   Loss: 0.657588\n",
            "Validation Loss: 0.767821\n",
            "Epoch 731\n",
            "Training   Loss: 0.657650\n",
            "Validation Loss: 0.767700\n",
            "Epoch 732\n",
            "Training   Loss: 0.657443\n",
            "Validation Loss: 0.767427\n",
            "Epoch 733\n",
            "Training   Loss: 0.657527\n",
            "Validation Loss: 0.767333\n",
            "Epoch 734\n",
            "Training   Loss: 0.657794\n",
            "Validation Loss: 0.767667\n",
            "Epoch 735\n",
            "Training   Loss: 0.657721\n",
            "Validation Loss: 0.767269\n",
            "Epoch 736\n",
            "Training   Loss: 0.657712\n",
            "Validation Loss: 0.766903\n",
            "Epoch 737\n",
            "Training   Loss: 0.657915\n",
            "Validation Loss: 0.767109\n",
            "Epoch 738\n",
            "Training   Loss: 0.657938\n",
            "Validation Loss: 0.766820\n",
            "Epoch 739\n",
            "Training   Loss: 0.657912\n",
            "Validation Loss: 0.766569\n",
            "Epoch 740\n",
            "Training   Loss: 0.657937\n",
            "Validation Loss: 0.766610\n",
            "Epoch 741\n",
            "Training   Loss: 0.658132\n",
            "Validation Loss: 0.766684\n",
            "Epoch 742\n",
            "Training   Loss: 0.658023\n",
            "Validation Loss: 0.766428\n",
            "Epoch 743\n",
            "Training   Loss: 0.658103\n",
            "Validation Loss: 0.766618\n",
            "Epoch 744\n",
            "Training   Loss: 0.657934\n",
            "Validation Loss: 0.766420\n",
            "Epoch 745\n",
            "Training   Loss: 0.657961\n",
            "Validation Loss: 0.766411\n",
            "Epoch 746\n",
            "Training   Loss: 0.658006\n",
            "Validation Loss: 0.766388\n",
            "Epoch 747\n",
            "Training   Loss: 0.657732\n",
            "Validation Loss: 0.766247\n",
            "Epoch 748\n",
            "Training   Loss: 0.658068\n",
            "Validation Loss: 0.766367\n",
            "Epoch 749\n",
            "Training   Loss: 0.657634\n",
            "Validation Loss: 0.765835\n",
            "Epoch 750\n",
            "Training   Loss: 0.657646\n",
            "Validation Loss: 0.765915\n",
            "Epoch 751\n",
            "Training   Loss: 0.657460\n",
            "Validation Loss: 0.765500\n",
            "Epoch 752\n",
            "Training   Loss: 0.657446\n",
            "Validation Loss: 0.765408\n",
            "Epoch 753\n",
            "Training   Loss: 0.657550\n",
            "Validation Loss: 0.765442\n",
            "Epoch 754\n",
            "Training   Loss: 0.657484\n",
            "Validation Loss: 0.765198\n",
            "Epoch 755\n",
            "Training   Loss: 0.657610\n",
            "Validation Loss: 0.765356\n",
            "Epoch 756\n",
            "Training   Loss: 0.657386\n",
            "Validation Loss: 0.765018\n",
            "Epoch 757\n",
            "Training   Loss: 0.657506\n",
            "Validation Loss: 0.764981\n",
            "Epoch 758\n",
            "Training   Loss: 0.657474\n",
            "Validation Loss: 0.764821\n",
            "Epoch 759\n",
            "Training   Loss: 0.657439\n",
            "Validation Loss: 0.764756\n",
            "Epoch 760\n",
            "Training   Loss: 0.657401\n",
            "Validation Loss: 0.764602\n",
            "Epoch 761\n",
            "Training   Loss: 0.657361\n",
            "Validation Loss: 0.764700\n",
            "Epoch 762\n",
            "Training   Loss: 0.657178\n",
            "Validation Loss: 0.764381\n",
            "Epoch 763\n",
            "Training   Loss: 0.657171\n",
            "Validation Loss: 0.764376\n",
            "Epoch 764\n",
            "Training   Loss: 0.657119\n",
            "Validation Loss: 0.764579\n",
            "Epoch 765\n",
            "Training   Loss: 0.657267\n",
            "Validation Loss: 0.764738\n",
            "Epoch 766\n",
            "Training   Loss: 0.657152\n",
            "Validation Loss: 0.764429\n",
            "Epoch 767\n",
            "Training   Loss: 0.657156\n",
            "Validation Loss: 0.764604\n",
            "Epoch 768\n",
            "Training   Loss: 0.657339\n",
            "Validation Loss: 0.764131\n",
            "Epoch 769\n",
            "Training   Loss: 0.657002\n",
            "Validation Loss: 0.763785\n",
            "Epoch 770\n",
            "Training   Loss: 0.657172\n",
            "Validation Loss: 0.763898\n",
            "Epoch 771\n",
            "Training   Loss: 0.657358\n",
            "Validation Loss: 0.763969\n",
            "Epoch 772\n",
            "Training   Loss: 0.657114\n",
            "Validation Loss: 0.763964\n",
            "Epoch 773\n",
            "Training   Loss: 0.657154\n",
            "Validation Loss: 0.763981\n",
            "Epoch 774\n",
            "Training   Loss: 0.657272\n",
            "Validation Loss: 0.764024\n",
            "Epoch 775\n",
            "Training   Loss: 0.657070\n",
            "Validation Loss: 0.763733\n",
            "Epoch 776\n",
            "Training   Loss: 0.657086\n",
            "Validation Loss: 0.763603\n",
            "Epoch 777\n",
            "Training   Loss: 0.656991\n",
            "Validation Loss: 0.763512\n",
            "Epoch 778\n",
            "Training   Loss: 0.657127\n",
            "Validation Loss: 0.763363\n",
            "Epoch 779\n",
            "Training   Loss: 0.657187\n",
            "Validation Loss: 0.762960\n",
            "Epoch 780\n",
            "Training   Loss: 0.657088\n",
            "Validation Loss: 0.762921\n",
            "Epoch 781\n",
            "Training   Loss: 0.657064\n",
            "Validation Loss: 0.763032\n",
            "Epoch 782\n",
            "Training   Loss: 0.656984\n",
            "Validation Loss: 0.763163\n",
            "Epoch 783\n",
            "Training   Loss: 0.657043\n",
            "Validation Loss: 0.763159\n",
            "Epoch 784\n",
            "Training   Loss: 0.656747\n",
            "Validation Loss: 0.763188\n",
            "Epoch 785\n",
            "Training   Loss: 0.656721\n",
            "Validation Loss: 0.763049\n",
            "Epoch 786\n",
            "Training   Loss: 0.656789\n",
            "Validation Loss: 0.762942\n",
            "Epoch 787\n",
            "Training   Loss: 0.656533\n",
            "Validation Loss: 0.763038\n",
            "Epoch 788\n",
            "Training   Loss: 0.656421\n",
            "Validation Loss: 0.762806\n",
            "Epoch 789\n",
            "Training   Loss: 0.656621\n",
            "Validation Loss: 0.762894\n",
            "Epoch 790\n",
            "Training   Loss: 0.656684\n",
            "Validation Loss: 0.762821\n",
            "Epoch 791\n",
            "Training   Loss: 0.656730\n",
            "Validation Loss: 0.762693\n",
            "Epoch 792\n",
            "Training   Loss: 0.656487\n",
            "Validation Loss: 0.762097\n",
            "Epoch 793\n",
            "Training   Loss: 0.656548\n",
            "Validation Loss: 0.762142\n",
            "Epoch 794\n",
            "Training   Loss: 0.656654\n",
            "Validation Loss: 0.762045\n",
            "Epoch 795\n",
            "Training   Loss: 0.656827\n",
            "Validation Loss: 0.762025\n",
            "Epoch 796\n",
            "Training   Loss: 0.656859\n",
            "Validation Loss: 0.761845\n",
            "Epoch 797\n",
            "Training   Loss: 0.657022\n",
            "Validation Loss: 0.761881\n",
            "Epoch 798\n",
            "Training   Loss: 0.657073\n",
            "Validation Loss: 0.761866\n",
            "Epoch 799\n",
            "Training   Loss: 0.657327\n",
            "Validation Loss: 0.762052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training History"
      ],
      "metadata": {
        "id": "JWkolgx3beqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_number = list(range((loss_train > 0.).sum()))\n",
        "plt.figure()\n",
        "plt.plot(epoch_number, loss_train[loss_train>0.],label='Training Loss')\n",
        "plt.plot(epoch_number, loss_val[loss_train>0.],label='Validation Loss')\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.grid(True)\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "5eqM86uEUrlb",
        "outputId": "83dc3243-24be-4343-e513-927d871dfd04"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this picture of the plot it is clearly visible that we need to add more epochs to make our model learns more about the data as the training loss and validation loss are very close but not 0."
      ],
      "metadata": {
        "id": "z4_b579ycOnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the ROC curves"
      ],
      "metadata": {
        "id": "Vb0UhI62blqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['gluon', 'quark', 'W', 'Z', 'top']\n",
        "lst = []\n",
        "n_batches_val = int(X_val.size()[0]/batch_size)\n",
        "if args_cuda:    \n",
        "    for j in torch.split(X_val, n_batches_val):\n",
        "        a = gnn(j).cpu().data.numpy()\n",
        "        lst.append(a)\n",
        "else:\n",
        "    for j in torch.split(X_val, n_batches_val):\n",
        "        a = gnn(j).cpu().data.numpy()\n",
        "        lst.append(a)\n",
        "predicted = Variable(torch.FloatTensor(np.concatenate(lst)))"
      ],
      "metadata": {
        "id": "XU4A9nPZbhAL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# there is no softmax in the output layer. We have to put it by \n",
        "predicted = torch.nn.functional.softmax(predicted, dim=1)"
      ],
      "metadata": {
        "id": "J9QR29GJbrka"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_val = predicted.data.numpy()\n",
        "true_val = y_val.cpu().data.numpy()"
      ],
      "metadata": {
        "id": "Z3EsJ-M7bv1q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "#### get the ROC curves\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "auc1 = {}\n",
        "plt.figure()\n",
        "for i, label in enumerate(labels):\n",
        "        fpr[label], tpr[label], threshold = roc_curve((true_val== i), predict_val[:,i])\n",
        "        auc1[label] = auc(fpr[label], tpr[label])\n",
        "        plt.plot(tpr[label],fpr[label],label='%s tagger, auc = %.1f%%'%(label,auc1[label]*100.))\n",
        "plt.semilogy()\n",
        "plt.xlabel(\"sig. efficiency\")\n",
        "plt.ylabel(\"bkg. mistag rate\")\n",
        "plt.ylim(0.001,1)\n",
        "plt.grid(True)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "68bi_1Sxbymh",
        "outputId": "d0384943-e778-40be-85ac-376e4be95de2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Hjv5pvwvb1hi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}